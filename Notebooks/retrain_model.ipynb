{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9be2a419-413b-46c8-ac1d-3b5b7102430a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f3453d-655d-454a-8139-9d19b35bb445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, get_scheduler\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f83ada-77de-4499-8d2a-31233c3dee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the XSum dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"EdinburghNLP/xsum\")\n",
    "\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_dataset = ds['train']\n",
    "eval_dataset = ds['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277757fb-6c1f-4d21-8014-b02d73407b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b22ea6f-c2f9-4cbe-86ad-80067eab1c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0405fc59964596bd810b24545ab7d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  95%|#########4| 2.16G/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987004bed783478f83461bbf44250fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained PEGASUS model and tokenizer\n",
    "model_name = \"google/pegasus-large\"  # You can also try other PEGASUS variants like \"google/pegasus-xsum\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e843b008-7b03-4928-875f-f73008e719ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfdc78d4-30c5-4411-a58c-77a5f46ff15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e28ad1a4ac46e0a255aa3bb8110825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/204045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praye\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8ac36d388043f78e4abfd7dfe3f44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_data(examples):\n",
    "    # Tokenize the inputs (documents)\n",
    "    model_inputs = tokenizer(examples['document'], max_length=1024, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for the labels (summaries)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['summary'], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Preprocess the datasets\n",
    "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
    "eval_dataset = eval_dataset.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08461600-183a-456b-a82b-ae21aea4ba1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define training arguments with evaluation every few steps\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model_checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Change to steps\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     eval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,  \u001b[38;5;66;03m# Evaluate every 1000 steps\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     save_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,  \u001b[38;5;66;03m# Save every 1000 steps\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m      9\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     10\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     11\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     13\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m     metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     greater_is_better\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32m<string>:132\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1750\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1753\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1754\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_compile_backend` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1756\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1757\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:2250\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2247\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2249\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_devices\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:60\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     58\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfget(obj)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:2123\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2123\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2124\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2125\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{ACCELERATE_MIN_VERSION}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2126\u001b[0m         )\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2128\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`"
     ]
    }
   ],
   "source": [
    "# Define training arguments with evaluation every few steps\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_checkpoints\",\n",
    "    evaluation_strategy=\"steps\",  # Change to steps\n",
    "    eval_steps=1000,  # Evaluate every 1000 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,  # Save every 1000 steps\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728dbc1f-37fd-488b-999c-2b1b9ea15c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up early stopping\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)  # Stop after 2 evaluations without improvement\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "def get_lr_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
    "    return get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd7ae7-1250-4fdc-bafb-5fdf0e94e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer with callbacks\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc43bdd-c4c0-4cbb-aa5e-796095e05649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after training\n",
    "model.save_pretrained(\"./pegasus_finetuned\")\n",
    "tokenizer.save_pretrained(\"./pegasus_finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa01a762-069e-4460-9f28-19db339b19b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aad85e8f-fbe7-4b6d-9c44-9bc51a844c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\praye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for PDF handling\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7473b732-34bc-4262-b066-381f1afbdd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_newspaper_dataset(train_data,eval_data):\n",
    "    # dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")  # Load the dataset\n",
    "    # Shuffle the data and split it into 90% train and 10% eval\n",
    "    # train_data = dataset['train'].shuffle(seed=42).select(range(27000))  # Select 27,000 samples for training\n",
    "    # eval_data = dataset['train'].shuffle(seed=42).select(range(27000, 30000))  # Select 3,000 samples for evaluation\n",
    "    return train_data, eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef199b3b-a033-4551-b052-76b2bca5c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune a summarization model using the dataset\n",
    "def train_summarization_model(train_data, eval_data, save_model_path):\n",
    "    model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "    tokenizer = BartTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "\n",
    "    def tokenize_data(examples):\n",
    "        inputs = examples['article']\n",
    "        model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "\n",
    "        # Tokenize labels\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(examples['highlights'], max_length=150, truncation=True, padding=\"max_length\")\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_train_data = train_data.map(tokenize_data, batched=True)\n",
    "    tokenized_eval_data = eval_data.map(tokenize_data, batched=True)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,  # Adjust batch size to fit your GPU memory\n",
    "        per_device_eval_batch_size=2,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        remove_unused_columns=False,\n",
    "        eval_steps=500,  # Evaluate every 500 steps\n",
    "        push_to_hub=False,\n",
    "        fp16=True,  # Use 16-bit precision to speed up training\n",
    "        logging_dir=\"./logs\",  # Directory for storing logs\n",
    "        logging_steps=100,  # Log every 100 steps\n",
    "    )\n",
    "\n",
    "    # Create Trainer object\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_data,\n",
    "        eval_dataset=tokenized_eval_data,  # Include eval_dataset\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model and tokenizer at the specified path\n",
    "    model.save_pretrained(save_model_path)\n",
    "    tokenizer.save_pretrained(save_model_path)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Summarize text using the fine-tuned model\n",
    "# def summarize_with_model(model, tokenizer, text, max_length=150, min_length=30):\n",
    "#     inputs = tokenizer([text], max_length=1024, return_tensors=\"pt\", truncation=True)\n",
    "#     summary_ids = model.generate(inputs[\"input_ids\"], max_length=max_length, min_length=min_length, num_beams=4, early_stopping=True)\n",
    "#     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "#     return summary\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"output_cleaned.pdf\"  # Path to the cleaned PDF\n",
    "save_model_path = \"/content/drive/MyDrive/Project-I-Model/Facebook-Bert\"  # Path where the model will be saved\n",
    "\n",
    "# Step 1: Extract and clean text from the PDF\n",
    "extracted_text = extract_and_clean_pdf_text(pdf_path)\n",
    "print(\"Extracted Text from PDF:\\n\", extracted_text[:500])  # Print first 500 characters for sanity check\n",
    "\n",
    "# Step 2: Load the small newspaper dataset (CNN/DailyMail)\n",
    "train_data, eval_data = load_newspaper_dataset(train_dataset,val_dataset)\n",
    "\n",
    "# Step 3: Fine-tune the summarization model and save it at the specified path\n",
    "model, tokenizer = train_summarization_model(train_data, eval_data, save_model_path)\n",
    "\n",
    "# # Step 4: Summarize the extracted PDF text using the fine-tuned model\n",
    "# summary = summarize_with_model(model, tokenizer, extracted_text)\n",
    "# print(\"Summary of PDF Text:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36329f-cc0f-4c92-a9c4-a47bb649f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_with_model(model, tokenizer, extracted_text)\n",
    "print(\"Summary of PDF Text:\\n\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba837ba-8c08-43b7-ae82-1ab1e5ce4344",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea3bcf90-0129-437d-933d-11e8a58a480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f03b0f83-2694-4933-8092-aebb9b0ff99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer from the local path\n",
    "local_model_path = \"Facebook-Bert-20241016T112414Z-001/Facebook-Bert\"  # e.g., \"./pegasus_finetuned\" or wherever your model is saved\n",
    "tokenizer = BartTokenizer.from_pretrained(local_model_path)\n",
    "model = BartForConditionalGeneration.from_pretrained(local_model_path)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eed01bef-7697-4895-af23-23e4acc1d146",
   "metadata": {},
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"device : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb403be3-299b-4837-b141-1822776b5673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: A Tutorial on Bayesian Optimization\n",
      "Peter I. Frazier\n",
      "July 10, 2018\n",
      "Abstract\n",
      "Bayesian optimization is an approach to optimizing objective functions that take a long time (min-\n",
      "utes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20\n",
      "dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective\n",
      "and quantiﬁes the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian\n",
      "process regression, and then uses an acquisition function deﬁned from this surrogate to decide where to\n",
      "sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process re-\n",
      "gression and three common acquisition functions: expected improvement, entropy search, and knowledge\n",
      "gradient. We then discuss more advanced techniques, including running multiple function evaluations\n",
      "in parallel, multi-ﬁdelity and multi-information source optimization, expensive-to-evaluate constraints,\n",
      "random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative infor-\n",
      "mation. We conclude with a discussion of Bayesian optimization software and future research directions\n",
      "in the ﬁeld. Within our tutorial material we provide a generalization of expected improvement to noisy\n",
      "evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is\n",
      "justiﬁed by a formal decision-theoretic argument, standing in contrast to previous ad hoc modiﬁcations.\n",
      "1\n",
      "Introduction\n",
      "Bayesian optimization (BayesOpt) is a class of machine-learning-based optimization methods focused on\n",
      "solving the problem\n",
      "max\n",
      "x∈A f(x),\n",
      "(1)\n",
      "where the feasible set and objective function typically have the following properties:\n",
      "• The input x is in Rd for a value of d that is not too large. Typically d ≤20 in most successful\n",
      "applications of BayesOpt.\n",
      "• The feasible set A is a simple set, in which it is easy to assess membership.\n",
      "Typically A is a\n",
      "hyper-rectangle {x ∈Rd : ai ≤xi ≤bi} or the d-dimensional simplex {x ∈Rd : P\n",
      "i xi = 1}. Later\n",
      "(Section 5) we will relax this assumption.\n",
      "• The objective function f is continuous. This will typically be required to model f using Gaussian\n",
      "process regression.\n",
      "• f is “expensive to evaluate” in the sense that the number of evaluations that may be performed is\n",
      "limited, typically to a few hundred. This limitation typically arises because each evaluation takes\n",
      "a substantial amount of time (typically hours), but may also occur because each evaluation bears\n",
      "a monetary cost (e.g., from purchasing cloud computing power, or buying laboratory materials),\n",
      "or an opportunity cost (e.g., if evaluating f requires asking a human subject questions who will\n",
      "tolerate only a limited number).\n",
      "• f lacks known special structure like concavity or linearity that would make it easy to optimize using\n",
      "techniques that leverage such structure to improve eﬃciency. We summarize this by saying f is a\n",
      "“black box.”\n",
      "• When we evaluate f, we observe only f(x) and no ﬁrst- or second-order derivatives. This prevents\n",
      "the application of ﬁrst- and second-order methods like gradient descent, Newton’s method, or quasi-\n",
      "Newton methods. We refer to problems with this property as “derivative-free”.\n",
      "• Through most of the article, we will assume f(x) is observed without noise. Later (Section 5) we\n",
      "will allow f(x) to be obscured by stochastic noise. In almost all work on Bayesian optimization,\n",
      "noise is assumed independent across evaluations and Gaussian with constant variance.\n",
      "1\n",
      "arXiv:1807.02811v1  [stat.ML]  8 Jul 2018\n",
      "\n",
      "• Our focus is on ﬁnding a global rather than local optimum.\n",
      "We summarize these problem characteristics by saying that BayesOpt is designed for black-box derivative-\n",
      "free global optimization.\n",
      "The ability to optimize expensive black-box derivative-free functions makes BayesOpt extremely ver-\n",
      "satile. Recently it has become extremely popular for tuning hyperparameters in machine learning al-\n",
      "gorithms, especially deep neural networks (Snoek et al., 2012). Over a longer period, since the 1960s,\n",
      "BayesOpt has been used extensively for designing engineering systems (Moˇckus, 1989; Jones et al., 1998;\n",
      "Forrester et al., 2008). BayesOpt has also been used to choose laboratory experiments in materials and\n",
      "drug design (Negoescu et al., 2011; Frazier and Wang, 2016; Packwood, 2017), in calibration of envi-\n",
      "ronmental models (Shoemaker et al., 2007), and in reinforcement learning (Brochu et al., 2009; Lizotte,\n",
      "2008; Lizotte et al., 2007).\n",
      "BayesOpt originated with the work of Kushner (Kushner, 1964), Zilinskas (ˇZilinskas, 1975; Moˇckus\n",
      "et al., 1978), and Moˇckus (Moˇckus, 1975; Moˇckus, 1989), but received substantially more attention\n",
      "after that work was popularized by Jones et al. (1998) and their work on the Eﬃcient Global Opti-\n",
      "mization (EGO) algorithm. Following Jones et al. (1998), innovations developed in that same literature\n",
      "include multi-ﬁdelity optimization (Huang et al., 2006; S´obester et al., 2004), multi-objective optimiza-\n",
      "tion (Keane, 2006; Knowles, 2006; Moˇckus and Moˇckus, 1991), and a study of convergence rates (Calvin,\n",
      "1997; Calvin and ˇZilinskas, 2000; Calvin and ˇZilinskas, 2005; Calvin and ˇZilinskas, 1999). The observa-\n",
      "tion made by Snoek et al. (2012) that BayesOpt is useful for training deep neural networks sparked a\n",
      "surge of interest within machine learning, with complementary innovations from that literature including\n",
      "multi-task optimization (Swersky et al., 2013; Toscano-Palmerin and Frazier, 2018), multi-ﬁdelity opti-\n",
      "mization speciﬁcally aimed at training deep neural networks (Klein et al., 2016), and parallel methods\n",
      "(Ginsbourger et al., 2007, 2010; Wang et al., 2016a; Wu and Frazier, 2016). Gaussian process regression,\n",
      "its close cousin kriging, and BayesOpt have also been studied recently in the simulation literature (Klei-\n",
      "jnen et al., 2008; Salemi et al., 2014; Mehdad and Kleijnen, 2018) for modeling and optimizing systems\n",
      "simulated using discrete event simulation.\n",
      "There are other techniques outside of BayesOpt that can be used to optimize expensive derivative-\n",
      "free black-box functions. While we do not review methods from this literature here in detail, many of\n",
      "them have a similar ﬂavor to BayesOpt methods: they maintain a surrogate that models the objective\n",
      "function, which they use to choose where to evaluate (Booker et al., 1999; Regis and Shoemaker, 2007b,a,\n",
      "2005). This more general class of methods is often called “surrogate methods.” Bayesian optimization\n",
      "distinguishes itself from other surrogate methods by using surrogates developed using Bayesian statistics,\n",
      "and in deciding where to evaluate the objective using a Bayesian interpretation of these surrogates.\n",
      "We ﬁrst introduce the typical form that Bayesian optimization algorithms take in Section 2. This\n",
      "form involves two primary components: a method for statistical inference, typically Gaussian process\n",
      "(GP) regression; and an acquisition function for deciding where to sample, which is often expected\n",
      "improvement.\n",
      "We describe these two components in detail in Sections 3 and 4.1.\n",
      "We then describe\n",
      "three alternate acquisition functions: knowledge-gradient (Section 4.2), entropy search, and predictive\n",
      "entropy search (Section 4.3). These alternate acquisition functions are particularly useful in problems\n",
      "falling outside the strict set of assumptions above, which we call “exotic” Bayesian optimization problems\n",
      "and we discuss in Section 5. These exotic Bayesian optimization problems include those with parallel\n",
      "evaluations, constraints, multi-ﬁdelity evaluations, multiple information sources, random environmental\n",
      "conditions, multi-task objectives, and derivative observations. We then discuss Bayesian optimization\n",
      "and Gaussian process regression software in Section 6 and conclude with a discussion of future research\n",
      "directions in Section 7.\n",
      "Other tutorials and surveys on Bayesian optimization include Shahriari et al. (2016); Brochu et al.\n",
      "(2009); Sasena (2002); Frazier and Wang (2016). This tutorial diﬀers from these others in its coverage\n",
      "of non-standard or “exotic” Bayesian optimization problems. It also diﬀers in its substantial emphasis\n",
      "on acquisition functions, with less emphasis on GP regression. Finally, it includes what we believe is a\n",
      "novel analysis of expected improvement for noisy measurements, and argues that the acquisition function\n",
      "previously proposed by Scott et al. (2011) is the most natural way to apply the expected improvement\n",
      "acquisition function when measurements are noisy.\n",
      "2\n",
      "Overview of BayesOpt\n",
      "BayesOpt consists of two main components: a Bayesian statistical model for modeling the objective\n",
      "function, and an acquisition function for deciding where to sample next. After evaluating the objective\n",
      "according to an initial space-ﬁlling experimental design, often consisting of points chosen uniformly at\n",
      "2\n",
      "\n",
      "random, they are used iteratively to allocate the remainder of a budget of N function evaluations, as\n",
      "shown in Algorithm 1.\n",
      "Algorithm 1 Basic pseudo-code for Bayesian optimization\n",
      "Place a Gaussian process prior on f\n",
      "Observe f at n0 points according to an initial space-ﬁlling experimental design. Set n = n0.\n",
      "while n ≤N do\n",
      "Update the posterior probability distribution on f using all available data\n",
      "Let xn be a maximizer of the acquisition function over x, where the acquisition function is computed using\n",
      "the current posterior distribution.\n",
      "Observe yn = f(xn).\n",
      "Increment n\n",
      "end while\n",
      "Return a solution: either the point evaluated with the largest f(x), or the point with the largest posterior\n",
      "mean.\n",
      "The statistical model, which is invariably a Gaussian process, provides a Bayesian posterior probability\n",
      "distribution that describes potential values for f(x) at a candidate point x. Each time we observe f at a\n",
      "new point, this posterior distribution is updated. We discuss Bayesian statistical modeling using Gaussian\n",
      "processes in detail in Section 3. The acquisition function measures the value that would be generated by\n",
      "evaluation of the objective function at a new point x, based on the current posterior distribution over\n",
      "f. We discuss expected improvement, the most commonly used acquisition function, in Section 4.1, and\n",
      "then discuss other acquisition functions in Section 4.2 and 4.3.\n",
      "One iteration of BayesOpt from Algorithm 1 using GP regression and expected improvement is\n",
      "illustrated in Figure 1. The top panel shows noise-free observations of the objective function with blue\n",
      "circles at three points. It also shows the output of GP regression. We will see below in Section 3 that\n",
      "GP regression produces a posterior probability distribution on each f(x) that is normally distributed\n",
      "with mean µn(x) and variance σ2\n",
      "n(x). This is pictured in the ﬁgure with µn(x) as the solid red line, and\n",
      "a 95% Bayesian credible interval for f(x), µn(x) ± 1.96 × σn(x), as dashed red lines. The mean can be\n",
      "interpreted as a point estimate of f(x). The credible interval acts like a conﬁdence interval in frequentist\n",
      "statistics, and contains f(x) with probability 95% according to the posterior distribution. The mean\n",
      "interpolates the previously evaluated points. The credible interval has 0 width at these points, and grows\n",
      "wider as we move away from them.\n",
      "The bottom panel shows the expected improvement acquisition function that corresponds to this\n",
      "posterior. Observe that it takes value 0 at points that have previously been evaluated. This is reason-\n",
      "able when evaluations of the objective are noise-free because evaluating these points provides no useful\n",
      "information toward solving (1). Also observe that it tends to be larger for points with larger credible\n",
      "intervals, because observing a point where we are more uncertain about the objective tends to be more\n",
      "useful in ﬁnding good approximate global optima. Also observe it tends to be larger for points with\n",
      "larger posterior means, because such points tend to be near good approximate global optima.\n",
      "We now discuss the components of BayesOpt in detail, ﬁrst discussing GP regression in Section 3,\n",
      "then discuss acquisition functions in Section 4, starting with expected improvement in Section 4.1. We\n",
      "then discuss more sophisticated acquisition functions (knowledge gradient, entropy search, and predictive\n",
      "entropy search) in Sections 4.2 and 4.3. Finally, we discuss extensions of the basic problem described\n",
      "in Section 1 in Section 5, discussing problems with measurement noise, parallel function evaluations,\n",
      "constraints, multi-ﬁdelity observations, and others.\n",
      "3\n",
      "Gaussian Process (GP) Regression\n",
      "GP regression is a Bayesian statistical approach for modeling functions. We oﬀer a brief introduction\n",
      "here. A more complete treatment may be found in Rasmussen and Williams (2006).\n",
      "We ﬁrst describe GP regression, focusing on f’s values at a ﬁnite collection of points x1, . . . , xk ∈Rd.\n",
      "It is convenient to collect the function’s values at these points together into a vector [f(x1), . . . , f(xk)].\n",
      "Whenever we have a quantity that is unknown in Bayesian statistics, like this vector, we suppose that it\n",
      "was drawn at random by nature from some prior probability distribution. GP regression takes this prior\n",
      "distribution to be multivariate normal, with a particular mean vector and covariance matrix.\n",
      "3\n",
      "\n",
      "Figure 1: Illustration of BayesOpt, maximizing an objective function f with a 1-dimensional continuous\n",
      "input. The top panel shows: noise-free observations of the objective function f at 3 points, in blue; an\n",
      "estimate of f(x) (solid red line); and Bayesian credible intervals (similar to conﬁdence intervals) for f(x)\n",
      "(dashed red line). These estimates and credible intervals are obtained using GP regression. The bottom panel\n",
      "shows the acquisition function. Bayesian optimization chooses to sample next at the point that maximizes\n",
      "the acquisition function, indicated here with an “x.”\n",
      "We construct the mean vector by evaluating a mean function µ0 at each xi.\n",
      "We construct the\n",
      "covariance matrix by evaluating a covariance function or kernel Σ0 at each pair of points xi, xj. The\n",
      "kernel is chosen so that points xi, xj that are closer in the input space have a large positive correlation,\n",
      "encoding the belief that they should have more similar function values than points that are far apart.\n",
      "The kernel should also have the property that the resulting covariance matrix is positive semi-deﬁnite,\n",
      "regardless of the collection of points chosen. Example mean functions and kernels are discussed below in\n",
      "Section 3.1.\n",
      "The resulting prior distribution on [f(x1), . . . , f(xk)] is,\n",
      "f(x1:k) ∼Normal (µ0(x1:k), Σ0(x1:k, x1:k)) ,\n",
      "(2)\n",
      "where we use compact notation for functions applied to collections of input points: x1:k indicates the\n",
      "sequence x1, . . . , xk, f(x1:k) = [f(x1), . . . , f(xk)], µ0(x1:k) = [µ0(x1), . . . , µ0(xk)], and Σ0(x1:k, x1:k) =\n",
      "[Σ0(x1, x1), . . . , Σ0(x1, xk); . . . ; Σ0(xk, x1), . . . , Σ0(xk, xk)].\n",
      "Suppose we observe f(x1:n) without noise for some n and we wish to infer the value of f(x) at some\n",
      "new point x. To do so, we let k = n + 1 and xk = x, so that the prior over [f(x1:n), f(x)] is given by\n",
      "(2). We may then compute the conditional distribution of f(x) given these observations using Bayes’\n",
      "rule (see details in Chapter 2.1 of Rasmussen and Williams (2006)),\n",
      "f(x)|f(x1:n) ∼Normal(µn(x), σ2\n",
      "n(x))\n",
      "µn(x) = Σ0(x, x1:n)Σ0(x1:n, x1:n)−1 (f(x1:n) −µ0(x1:n)) + µ0(x)\n",
      "σ2\n",
      "n(x) = Σ0(x, x) −Σ0(x, x1:n)Σ0(x1:n, x1:n)−1Σ0(x1:n, x).\n",
      "(3)\n",
      "This conditional distribution is called the posterior probability distribution in the nomenclature of\n",
      "4\n",
      "\n",
      "Figure 2: Random functions f drawn from a Gaussian process prior with a power exponential kernel. Each\n",
      "plot corresponds to a diﬀerent value for the parameter α1, with α1 decreasing from left to right. Varying\n",
      "this parameter creates diﬀerent beliefs about how quickly f(x) changes with x.\n",
      "Bayesian statistics. The posterior mean µn(x) is a weighted average between the prior µ0(x) and an\n",
      "estimate based on the data f(x1:n), with a weight that depends on the kernel. The posterior variance\n",
      "σ2\n",
      "n(x) is equal to the prior covariance Σ0(x, x) less a term that corresponds to the variance removed by\n",
      "observing f(x1:n).\n",
      "Rather than computing posterior means and variances directly using (3) and matrix inversion, it is\n",
      "typically faster and more numerically stable to use a Cholesky decomposition and then solve a linear\n",
      "system of equations. This more sophisticated technique is discussed as Algorithm 2.1 in Section 2.2 of\n",
      "Rasmussen and Williams (2006). Additionally, to improve the numerical stability of this approach or\n",
      "direct computation using (3), it is often useful to add a small positive number like 10−6 to each element\n",
      "of the diagonal of Σ0(x1:n, x1:n), especially when x1:n contains two or more points that are close together.\n",
      "This prevents eigenvalues of Σ0(x1:n, x1:n) from being too close to 0, and only changes the predictions\n",
      "that would be made by an inﬁnite-precision computation by a small amount.\n",
      "Although we have modeled f at only a ﬁnite number of points, the same approach can be used when\n",
      "modeling f over a continuous domain A. Formally a Gaussian process with mean function µ0 and kernel\n",
      "Σ0 is a probability distribution over the function f with the property that, for any given collection of\n",
      "points x1:k, the marginal probability distribution on f(x1:k) is given by (2). Moreover, the arguments\n",
      "that justiﬁed (3) still hold when our prior probability distribution on f is a Gaussian process.\n",
      "In addition to calculating the conditional distribution of f(x) given f(x1:n), it is also possible to\n",
      "calculate the conditional distribution of f at more than one unevaluated point. The resulting distribution\n",
      "is multivariate normal, with a mean vector and covariance kernel that depend on the location of the\n",
      "unevaluated points, the locations of the measured points x1:n, and their measured values f(x1:n). The\n",
      "functions that give entries in this mean vector and covariance matrix have the form required for a mean\n",
      "function and kernel described above, and the conditional distribution of f given f(x1:n) is a Gaussian\n",
      "process with this mean function and covariance kernel.\n",
      "3.1\n",
      "Choosing a Mean Function and Kernel\n",
      "We now discuss the choice of kernel.\n",
      "Kernels typically have the property that points closer in the\n",
      "input space are more strongly correlated, i.e., that if ||x −x′|| < ||x −x′′|| for some norm || · ||, then\n",
      "Σ0(x, x′) > Σ0(x, x′′). Additionally, kernels are required to be positive semi-deﬁnite functions. Here we\n",
      "describe two example kernels and how they are used.\n",
      "One commonly used and simple kernel is the power exponential or Gaussian kernel,\n",
      "Σ0(x, x′) = α0 exp\n",
      "\u0000−||x −x′||2\u0001\n",
      ",\n",
      "where ||x −x′||2 = Pd\n",
      "i=1 αi(xi −x′\n",
      "i)2, and α0:d are parameters of the kernel. Figure 2 shows random\n",
      "functions with a 1-dimensional input drawn from a Gaussian process prior with a power exponential\n",
      "kernel with diﬀerent values of α1. Varying this parameter creates diﬀerent beliefs about how quickly\n",
      "f(x) changes with x.\n",
      "Another commonly used kernel is the M`atern kernel,\n",
      "Σ0(x, x′) = α0 21−ν\n",
      "Γ(ν)\n",
      "\u0010√\n",
      "2ν||x −x′||\n",
      "\u0011ν\n",
      "Kν(\n",
      "√\n",
      "2ν||x −x′||)\n",
      "5\n",
      "\n",
      "where Kν is the modiﬁed Bessel function, and we have a parameter ν in addition to the parameters α0:d.\n",
      "We discuss choosing these parameters below in Section 3.2.\n",
      "Perhaps the most common choice for the mean function is a constant value, µ0(x) = µ. When f is\n",
      "believed to have a trend or some application-speciﬁc parametric structure, we may also take the mean\n",
      "function to be\n",
      "µ0(x) = µ +\n",
      "p\n",
      "X\n",
      "i=1\n",
      "βiΨi(x),\n",
      "(4)\n",
      "where each Ψi is a parametric function, and often a low-order polynomial in x.\n",
      "3.2\n",
      "Choosing Hyperparameters\n",
      "The mean function and kernel contain parameters.\n",
      "We typically call these parameters of the prior\n",
      "hyperparameters.\n",
      "We indicate them via a vector η.\n",
      "For example, if we use a M`atern kernel and a\n",
      "constant mean function, η = (α0:d, ν, µ).\n",
      "To choose the hyperparameters, three approaches are typically considered. The ﬁrst is to ﬁnd the\n",
      "maximum likelihood estimate (MLE). In this approach, when given observations f(x1:n), we calculate the\n",
      "likelihood of these observations under the prior, P(f(x1:n)|η), where we modify our notation to indicate\n",
      "its dependence on η. This likelihood is a multivariate normal density. Then, in maximum likelihood\n",
      "estimation, we set η to the value that maximizes this likelihood,\n",
      "ˆη = argmax\n",
      "η\n",
      "P(f(x1:n)|η)\n",
      "The second approach amends this ﬁrst approach by imagining that the hyperparameters η were\n",
      "themselves chosen from a prior, P(η). We then estimate η by the maximum a posteriori (MAP) estimate\n",
      "(Gelman et al., 2014), which is the value of η that maximizes the posterior,\n",
      "ˆη = argmax\n",
      "η\n",
      "P(η|f(x1:n)) = argmax\n",
      "η\n",
      "P(f(x1:n)|η)P(η)\n",
      "In moving from the ﬁrst expression to the second we have used Bayes’ rule and then dropped a normal-\n",
      "ization constant\n",
      "R\n",
      "P(f(x1:n)|η′)P(η′) dη′ that does not depend on the quantity η being optimized.\n",
      "The MLE is a special case of the MAP if we take the prior on the hyperparameters P(η) to be the\n",
      "(possibly degenerate) probability distribution that has constant density over the domain of η. The MAP is\n",
      "useful if the MLE sometimes estimates unreasonable hyperparameter values, for example, corresponding\n",
      "to functions that vary too quickly or too slowly (see Figure 2). By choosing a prior that puts more\n",
      "weight on hyperparameter values that are reasonable for a particular problem, MAP estimates can\n",
      "better correspond to the application. Common choices for the prior include the uniform distribution\n",
      "(for preventing estimates from falling outside of some pre-speciﬁed range), the normal distribution (for\n",
      "suggesting that the estimates fall near some nominal value without setting a hard cutoﬀ), and the log-\n",
      "normal and truncated normal distributions (for providing a similar suggestion for positive parameters).\n",
      "The third approach is called the fully Bayesian approach. In this approach, we wish to compute the\n",
      "posterior distribution on f(x) marginalizing over all possible values of the hyperparameters,\n",
      "P(f(x) = y|f(x1:n)) =\n",
      "Z\n",
      "P(f(x) = y|f(x1:n), η)P(η|f(x1:n)) dη\n",
      "(5)\n",
      "This integral is typically intractable, but we can approximate it through sampling:\n",
      "P(f(x) = y|f(x1:n)) ≈1\n",
      "J\n",
      "J\n",
      "X\n",
      "j=1\n",
      "P(f(x) = y|f(x1:n), η = ˆηj)\n",
      "(6)\n",
      "where (ˆηj : j = 1, . . . , J) are sampled from P(η|f(x1:n)) via an MCMC method, e.g., slice sampling (Neal,\n",
      "2003). MAP estimation can be seen as an approximation to fully Bayesian inference: if we approximate\n",
      "the posterior P(η|f(x1:n)) by a point mass at the η that maximizes the posterior density, then inference\n",
      "with the MAP recovers (5).\n",
      "6\n",
      "\n",
      "4\n",
      "Acquisition Functions\n",
      "Having surveyed Gaussian processes, we return to Algorithm 1 and discuss the acquisition function used\n",
      "in that loop. We focus on the setting described in Section 1 with noise-free evaluations, which we call\n",
      "the “standard” problem, and then discuss noisy evaluations, parallel evaluations, derivative observations,\n",
      "and other “exotic” extensions in Section 5.\n",
      "The most commonly used acquisition function is expected improvement, and we discuss it ﬁrst, in\n",
      "Section 4.1. Expected improvement performs well and is easy to use. We then discuss the knowledge\n",
      "gradient (Section 4.2), entropy search and predictive entropy search (Section 4.3) acquisition functions.\n",
      "These alternate acquisition functions are most useful in exotic problems where an assumption made by\n",
      "expected improvement, that the primary beneﬁt of sampling occurs through an improvement at the point\n",
      "sampled, is no longer true.\n",
      "4.1\n",
      "Expected Improvement\n",
      "The expected improvement acquisition function is derived by a thought experiment. Suppose we are\n",
      "using Algorithm 1 to solve (1), in which xn indicates the point sampled at iteration n and yn indicates\n",
      "the observed value. Assume that we may only return a solution that we have evaluated as our ﬁnal\n",
      "solution to (1). Also suppose for the moment that we have no evaluations left to make, and must return\n",
      "a solution based on those we have already performed. Since we observe f without noise, the optimal\n",
      "choice is the previously evaluated point with the largest observed value. Let f ∗\n",
      "n = maxm≤n f(xm) be the\n",
      "value of this point, where n is the number of times we have evaluated f thus far.\n",
      "Now suppose in fact we have one additional evaluation to perform, and we can perform it anywhere.\n",
      "If we evaluate at x, we will observe f(x). After this new evaluation, the value of the best point we have\n",
      "observed will either be f(x) (if f(x) ≥f ∗\n",
      "n) or f ∗\n",
      "n (if f(x) ≤f ∗\n",
      "n). The improvement in the value of the\n",
      "best observed point is then f(x) −f ∗\n",
      "n if this quantity is positive, and 0 otherwise. We can write this\n",
      "improvement more compactly as [f(x) −f ∗\n",
      "n]+, where a+ = max(a, 0) indicates the positive part.\n",
      "While we would like to choose x so that this improvement is large, f(x) is unknown until after the\n",
      "evaluation. What we can do, however, is to take the expected value of this improvement and choose x\n",
      "to maximize it. We deﬁne the expected improvement as,\n",
      "EIn(x) := En\n",
      "\u0002\n",
      "[f(x) −f ∗\n",
      "n]+\u0003\n",
      "(7)\n",
      "Here, En[·] = E[·|x1:n, y1:n] indicates the expectation taken under the posterior distribution given eval-\n",
      "uations of f at x1, . . . xn. This posterior distribution is given by (3): f(x) given x1:n, y1:n is normally\n",
      "distributed with mean µn(x) and variance σ2\n",
      "n(x).\n",
      "The expected improvement can be evaluated in closed form using integration by parts, as described\n",
      "in Jones et al. (1998) or Clark (1961). The resulting expression is\n",
      "EIn(x) = [∆n(x)]+ + σn(x)ϕ\n",
      "\u0012∆n(x)\n",
      "σn(x)\n",
      "\u0013\n",
      "−|∆n(x)|Φ\n",
      "\u0012∆n(x)\n",
      "σn(x)\n",
      "\u0013\n",
      ",\n",
      "(8)\n",
      "where ∆n(x) := µn(x) −f ∗\n",
      "n is the expected diﬀerence in quality between the proposed point x and the\n",
      "previous best.\n",
      "The expected improvement algorithm then evaluates at the point with the largest expected improve-\n",
      "ment,\n",
      "xn+1 = argmax EIn(x),\n",
      "(9)\n",
      "breaking ties arbitrarily. This algorithm was ﬁrst proposed by Moˇckus (Moˇckus, 1975) but was popu-\n",
      "larized by Jones et al. (1998). The latter article also used the name “Eﬃcient Global Optimization” or\n",
      "EGO.\n",
      "Implementations use a variety of approaches for solving (9). Unlike the objective f in our original\n",
      "optimization problem (1), EIn(x) is inexpensive to evaluate and allows easy evaluation of ﬁrst- and second-\n",
      "order derivatives. Implementations of the expected improvement algorithm can then use a continuous\n",
      "ﬁrst- or second-order optimization method to solve (9). For example, one technique that has worked\n",
      "well for the author is to calculate ﬁrst derivatives and use the quasi-Newton method L-BFGS-B (Liu and\n",
      "Nocedal, 1989).\n",
      "Figure 3 shows the contours of EIn(x) in terms of ∆n(x) and the posterior standard deviation σn(x).\n",
      "EIn(x) is increasing in both ∆n(x) and σn(x). Curves of ∆n(x) versus σn(x) with equal EI show how EI\n",
      "balances between evaluating at points with high expected quality (high ∆n(x))) versus high uncertainty\n",
      "(high σn(x)). In the context of optimization, evaluating at points with high expected quality relative\n",
      "7\n",
      "\n",
      "Figure 3: Contour plot of EI(x), the expected improvement (8), in terms of ∆n(x) (the expected diﬀerence\n",
      "in quality between the proposed point and the best previously evaluated point) and the posterior standard\n",
      "deviation σn(x). Blue indicates smaller values and red higher ones. The expected improvement is increasing\n",
      "in both quantities, and curves of ∆n(x) versus σn(x) with equal EI deﬁne an implicit tradeoﬀbetween\n",
      "evaluating at points with high expected quality (high ∆n(x) versus high uncertainty (high σn(x)).\n",
      "to the previous best point is valuable because good approximate global optima are likely to reside at\n",
      "such points. On the other hand, evaluating at points with high uncertainty is valuable because it teaches\n",
      "about the objective in locations where we have little knowledge and which tend to be far away from where\n",
      "we have previously measured. A point that is substantially better than one we have seen previously may\n",
      "very well reside there.\n",
      "Figure 1 shows EI(x) in the bottom panel. We see this tradeoﬀ, with the largest expected improvement\n",
      "occurring where the posterior standard deviation is high (far away from previously evaluated points),\n",
      "and where the posterior mean is also high. The smallest expected improvement is 0, at points where we\n",
      "have previously evaluated. The posterior standard deviation is 0 at this point, and the posterior mean\n",
      "is necessarily no larger than the best previously evaluated point. The expected improvement algorithm\n",
      "would evaluate next at the point indicated with an x, where EI is maximized.\n",
      "Choosing where to evaluate based on a tradeoﬀbetween high expected performance and high un-\n",
      "certainty appears in other domains, including multi-armed bandits Mahajan and Teneketzis (2008) and\n",
      "reinforcement learning Sutton and Barto (1998), and is often called the “exploration vs. exploitation\n",
      "tradeoﬀ” (Kaelbling et al., 1996).\n",
      "4.2\n",
      "Knowledge Gradient\n",
      "The knowledge-gradient acquisition function is derived by revisiting the assumption made in EI’s deriva-\n",
      "tion that we are only willing to return a previously evaluated point as our ﬁnal solution. That assumption\n",
      "is reasonable when evaluations are noise-free and we are highly risk averse, but if the decision-maker is\n",
      "willing to tolerate some risk then she might be willing to report a ﬁnal solution that has some uncertainty\n",
      "attached to it. Moreover, if evaluations have noise (discussed below in Section 5) then the ﬁnal solution\n",
      "reported will necessarily have uncertain value because we can hardly evaluate it an inﬁnite number of\n",
      "times.\n",
      "We replace this assumption by allowing the decision-maker to return any solution she likes, even\n",
      "if it has not been previously evaluated. We also assume risk-neutrality (Berger, 2013), i.e., we value a\n",
      "random outcome X according to its expected value. The solution that we would choose if we were to stop\n",
      "sampling after n samples would be the one with the largest µn(x) value. This solution (call it c\n",
      "x∗, since\n",
      "it approximates the global optimum x∗) would have value f(c\n",
      "x∗). f(c\n",
      "x∗) is random under the posterior,\n",
      "and has conditional expected value µn(c\n",
      "x∗) = maxx′ µn(x′) =: µ∗\n",
      "n.\n",
      "On the other hand, if we were to take one more sample at x, we would obtain a new posterior\n",
      "distribution with posterior mean µn+1(·). This posterior mean would be computed via (3), but including\n",
      "the additional observation xn+1, yn+1. If we were to report a ﬁnal solution after this sample, its expected\n",
      "8\n",
      "\n",
      "value under the new posterior distribution would be µ∗\n",
      "n+1 := maxx′ µn+1(x′).\n",
      "Thus, the increase in\n",
      "conditional expected solution value due to sampling is µ∗\n",
      "n+1 −µ∗\n",
      "n.\n",
      "While this quantity is unknown before we sample at xn+1, we can compute its expected value given\n",
      "the observations at x1, . . . , xn that we have.\n",
      "We call this quantity the knowledge gradient (KG) for\n",
      "measuring at x,\n",
      "KGn(x) := En [µ∗\n",
      "n+1 −µ∗\n",
      "n|xn+1 = x] .\n",
      "(10)\n",
      "Using the knowledge gradient as our acquisition function then leads us to sample at the point with largest\n",
      "KGn(x), argmaxx KGn(x).\n",
      "This algorithm was ﬁrst proposed in Frazier et al. (2009) for GP regression over discrete A, building\n",
      "on earlier work (Frazier et al., 2008) that proposed the same algorithm for Bayesian ranking and selection\n",
      "(Chick and Inoue, 2001) with an independent prior. (Bayesian ranking and selection is similar to Bayesian\n",
      "optimization, except that A is discrete and ﬁnite, observations are necessarily noisy, and the prior is\n",
      "typically independent across x.)\n",
      "The conceptually simplest way to compute the KG acquisition function is via simulation, as shown\n",
      "in Algorithm 2. Within a loop, this algorithm simulates one possible value for the observation yn+1 that\n",
      "may result from taking evaluation n + 1 at a designated x. Then it computes what the maximum of\n",
      "the new posterior mean µ∗\n",
      "n+1 would be if that value for yn+1 were the one that actually resulted from\n",
      "the measurement. It then subtracts µ∗\n",
      "n to obtain the corresponding increase in solution quality. This\n",
      "comprises one loop of the algorithm. It iterates this loop many (J) times and averages the diﬀerences\n",
      "µ∗\n",
      "n+1 −µ∗\n",
      "n obtained from diﬀerent simulated values for yn+1 to estimate the KG acquisition function\n",
      "KGn(x). As J grows large, this estimate converges to KGn(x).\n",
      "In principle, this algorithm can be used to evaluate KGn(x) within a derivative-free simulation-based\n",
      "optimization method to optimize the KG acquisition function. However, optimizing noisy simulation-\n",
      "based functions without access to derivatives is challenging. Frazier et al. (2009) proposed discretizing\n",
      "A and calculating (10) exactly using properties of the normal distribution. This works well for low-\n",
      "dimensional problems but becomes computationally burdensome in higher dimensions.\n",
      "Algorithm 2 Simulation-based computation of the knowledge-gradient factor KGn(x).\n",
      "Let µ∗\n",
      "n = maxx′ µn(x′).\n",
      "(When calculating µ∗\n",
      "n and µ∗\n",
      "n+1 below, use a nonlinear optimization method like L-BFGS.)\n",
      "for j = 1 to J: do\n",
      "Generate yn+1 ∼Normal(µn(x), σ2\n",
      "n(x)). (Equivalently, Z ∼Normal(0, 1) and yn+1 = µn(x) + σn(x)Z.)\n",
      "Set µn+1(x′; x, yn+1) to the posterior mean at x′ via (3) with (x, yn+1) as the last observation.\n",
      "µ∗\n",
      "n+1 = maxx′ µn+1(x′; x, yn+1).\n",
      "∆(j) = µ∗\n",
      "n+1 −µ∗\n",
      "n.\n",
      "end for\n",
      "Estimate KGn(x) by 1\n",
      "J\n",
      "PJ\n",
      "j=1 ∆(j).\n",
      "Overcoming this challenge with dimensionality, Wu and Frazier (2016) proposed1 a substantially\n",
      "more eﬃcient and scalable approach, based on multi-start stochastic gradient ascent. Stochastic gradient\n",
      "ascent (Robbins and Monro, 1951; Blum, 1954) is an algorithm for ﬁnding local optima of functions used\n",
      "such unbiased gradient estimates widely used in machine learning (Bottou, 2012). Multistart stochastic\n",
      "gradient ascent (Mart´ı et al., 2016) runs multiple instances of stochastic gradient ascent from diﬀerent\n",
      "starting points and selects the best local optimum found as an approximate global optimum.\n",
      "We summarize this approach for maximizing the KG acquisition function in Algorithm 3. The al-\n",
      "gorithm iterates over starting points, indexed by r, and for each maintains a sequence of iterates x(r)\n",
      "t ,\n",
      "indexed by t, that converges to a local optimum of the KG acquisition function. The inner loop over t\n",
      "relies on a stochastic gradient G, which is a random variable whose expected value is equal to the gradient\n",
      "of the KG acquisition function with respect to where we sample, evaluated at the current iterate x(r)\n",
      "t−1.\n",
      "We obtain the next iterate by taking a step in the direction of the stochastic gradient G. The size of this\n",
      "step is determined by the magnitude of G and a decreasing step size αt. Once stochastic gradient ascent\n",
      "has run for T iterations for each start, Algorithm 3 uses simulation (Algorithm 2) to evaluate the KG\n",
      "acquisition function for the ﬁnal point obtained from each starting point, and selects the best one.\n",
      "1This approach was proposed in the context of BayesOpt with parallel function evaluations, but can also be used in the\n",
      "setting considered here in which we perform one evaluation at a time.\n",
      "9\n",
      "\n",
      "Algorithm 3 Eﬃcient method for ﬁnding x with the largest KGn(x), based on multistart stochastic gradient\n",
      "ascent. Takes as input a number of starts R, a number of iterations T for each pass of stochastic gradient\n",
      "ascent, a parameter a used to deﬁne a stepsize sequence, and a number of replications J. Suggested input\n",
      "parameters: R = 10, T = 102, a = 4, J = 103.\n",
      "for r = 1 to R do\n",
      "Choose x(r)\n",
      "0\n",
      "uniformly at random from A.\n",
      "for t = 1 to T do\n",
      "Let G be the stochastic gradient estimate of ∇KGn(x(r)\n",
      "t−1) from Algorithm 4.\n",
      "Let αt = a/(a + t).\n",
      "x(r)\n",
      "t\n",
      "= x(r)\n",
      "t−1 + αtG.\n",
      "end for\n",
      "Estimate KGn(x(r)\n",
      "T ) using Algorithm 2 and J replications.\n",
      "end for\n",
      "Return the x(r)\n",
      "T\n",
      "with the largest estimated value of KGn(x(r)\n",
      "T ).\n",
      "This stochastic gradient G used by the inner loop of Algorithm 3 is calculated via Algorithm 4. This\n",
      "algorithm is based on the idea that we can exchange gradient and expectation (under suﬃcient regularity\n",
      "conditions) to write,\n",
      "∇KGn(x) = ∇En [µ∗\n",
      "n+1 −µ∗\n",
      "n|xn+1 = x] . = En [∇µ∗\n",
      "n+1|xn+1 = x] ,\n",
      "where we have noted that µ∗\n",
      "n does not depend on x. This approach is called inﬁnitesimal perturbation\n",
      "analysis (Ho et al., 1983). Thus, to construct a stochastic gradient it is suﬃcient to sample ∇µ∗\n",
      "n+1.\n",
      "In other words, imagine ﬁrst sampling Z in the inner loop in Algorithm 2, and then holding Z ﬁxed\n",
      "while calculating the gradient of µ∗\n",
      "n+1 with respect to x. To calculate this gradient, see that µ∗\n",
      "n+1 is a\n",
      "maximum over x′ of µn+1(x′; x, yn+1) = µn+1(x′; x, µn(x) + σn(x)Z). This is a maximum over collection\n",
      "of functions of x. The envelope theorem (Milgrom and Segal, 2002) tells us (under suﬃcient regularity\n",
      "conditions) that the gradient with respect to x of a maximum of a collection of functions of x is given\n",
      "simply by ﬁrst ﬁnding the maximum in this collection, and then diﬀerentiating this single function with\n",
      "respect to x. In our setting, we apply this by letting c\n",
      "x∗be the x′ maximizing µn+1(x′; x, µn(x)+σn(x)Z),\n",
      "and then calculating the gradient of µn+1(c\n",
      "x∗; x, µn(x)+σn(x)Z) with respect to x while holding c\n",
      "x∗ﬁxed.\n",
      "In other words,\n",
      "∇max\n",
      "x′\n",
      "µn+1(x′; x, µn(x) + σn(x)Z) = ∇µn+1(c\n",
      "x∗; x, µn(x) + σn(x)Z),\n",
      "where we remind the reader that ∇refers to taking the gradient with respect to x, here and throughout.\n",
      "This is summarized in Algorithm 4.\n",
      "Algorithm 4 Simulation of unbiased stochastic gradients G with E[G] = ∇KGn(x). This stochastic gradient\n",
      "can then be used within stochastic gradient ascent to optimize the KG acquisition function.\n",
      "for j = 1 to J do\n",
      "Generate Z ∼Normal(0, 1)\n",
      "yn+1 = µn(x) + σn(x)Z.\n",
      "Let µn+1(x′; x, yn+1) = µn+1(x′; x, µn(x) + σn(x)Z) be the posterior mean at x′ computed via (3) with\n",
      "(x, yn+1) as the last observation.\n",
      "Solve maxx′ µn+1(x′; x, yn+1), e.g., using L-BFGS. Let c\n",
      "x∗be the maximizing x′.\n",
      "Let G(j) be the gradient of µn+1(c\n",
      "x∗; x, µn(x) + σn(x)Z) with respect to x, holding c\n",
      "x∗ﬁxed.\n",
      "end for\n",
      "Estimate ∇KGn(x) by G = 1\n",
      "J\n",
      "PJ\n",
      "j=1 G(j).\n",
      "Unlike expected improvement, which only considers the posterior at the point sampled, the KG\n",
      "acquisition considers the posterior over f’s full domain, and how the sample will change that posterior.\n",
      "KG places a positive value on measurements that cause the maximum of the posterior mean to improve,\n",
      "even if the value at the sampled point is not better than the previous best point. This provides a small\n",
      "performance beneﬁt in the standard BayesOpt problem with noise-free evaluations (Frazier et al., 2009),\n",
      "10\n",
      "\n",
      "and provides substantial performance improvements in problems with noise, multi-ﬁdelity observations,\n",
      "derivative observations, the need to integrate over environmental conditions, and other more exotic\n",
      "problem features (Section 5). In these alternate problems, the value of sampling comes not through an\n",
      "improvement in the best solution at the sampled point, but through an improvement in the maximum\n",
      "of the posterior mean across feasible solutions. For example, a derivative observation may show that\n",
      "the function is increasing in a particular direction in the vicinity of the sampled point. This may cause\n",
      "the maximum of the posterior mean to be substantially larger than the previous maximum, even if\n",
      "the function value at the sampled point is worse than the best previously sampled point. When such\n",
      "phenomenon are ﬁrst-order, KG tends to signiﬁcantly outperform EI (Wu et al., 2017; Poloczek et al.,\n",
      "2017; Wu and Frazier, 2016; Toscano-Palmerin and Frazier, 2018).\n",
      "4.3\n",
      "Entropy Search and Predictive Entropy Search\n",
      "The entropy search (ES) (Hennig and Schuler, 2012) acquisition function values the information we have\n",
      "about the location of the global maximum according to its diﬀerential entropy. ES seeks the point to eval-\n",
      "uate that causes the largest decrease in diﬀerential entropy. (Recall from, e.g., Cover and Thomas (2012),\n",
      "that the diﬀerential entropy of a continuous probability distribution p(x) is\n",
      "R\n",
      "p(x) log(p(x)) dx, and that\n",
      "smaller diﬀerential entropy indicates less uncertainty.)\n",
      "Predictive entropy search (PES) (Hern´andez-\n",
      "Lobato et al., 2014) seeks the same point, but uses a reformulation of the entropy reduction objective\n",
      "based on mutual information. Exact calculations of PES and ES would give equivalent acquisition func-\n",
      "tions, but exact calculation is not typically possible, and so the diﬀerence in computational techniques\n",
      "used to approximate the PES and ES acquisition functions creates practical diﬀerences in the sampling\n",
      "decisions that result from the two approaches. We ﬁrst discuss ES and then PES.\n",
      "Let x∗be the global optimum of f. The posterior distribution on f at time n induces a probability\n",
      "distribution for x∗. Indeed, if the domain A were ﬁnite, then we could represent f over its domain by a\n",
      "vector (f(x) : x ∈A), and x∗would correspond to the largest element in this vector. The distribution of\n",
      "this vector under the time-n posterior distribution would be multivariate normal, and this multivariate\n",
      "normal distribution would imply the distribution of x∗. When A is continuous, the same ideas apply,\n",
      "where x∗is a random variable whose distribution is implied by the Gaussian process posterior on f.\n",
      "With this understanding, we represent the entropy of the time-n posterior distribution on x∗with\n",
      "the notation H(Pn(x∗)). Similarly, H(Pn(x∗|x, f(x))) represents the entropy of what the time-n + 1\n",
      "posterior distribution on x∗will be if we observe at x and see f(x). This quantity depends on the value\n",
      "of f(x) observed. Then, the entropy reduction due to sampling x can be written,\n",
      "ESn(x) = H(Pn(x∗)) −Ef(x) [H(Pn(x∗|f(x)))] .\n",
      "(11)\n",
      "In the second term, the subscript in the outer expectation indicates that we take the expectation over f(x).\n",
      "Equivalently, this can be written\n",
      "R\n",
      "ϕ(y; µn(x), σ2\n",
      "n(x))H(Pn(x∗|f(x) = y)) dy where ϕ(y; µn(x), σ2\n",
      "n(x)) is\n",
      "the normal density with mean µn(x) and variance σ2\n",
      "n(x).\n",
      "Like KG, ES and PES below are inﬂuenced by how the measurement changes the posterior over the\n",
      "whole domain, and not just on whether it improves over an incumbent solution at the point sampled.\n",
      "This is useful when deciding where to sample in exotic problems, and it is here that ES and PES can\n",
      "provide substantial value relative to EI.\n",
      "While ES can be computed and optimized approximately (Hennig and Schuler, 2012), doing so is\n",
      "challenging because (a) the entropy of the maximizer of a Gaussian process is not available in closed\n",
      "form; (b) we must calculate this entropy for a large number of y to approximate the expectation in (11);\n",
      "and (c) we must then optimize this hard-to-evaluate function. Unlike KG, there is no known method for\n",
      "computing stochastic gradients that would simplify this optimization.\n",
      "PES oﬀers an alternate approach for computing (11). This approach notes that the reduction in the\n",
      "entropy of x∗due to measuring f(x) is equal to the mutual information between f(x) and x∗, which is\n",
      "in turn equal to the reduction in the entropy of f(x) due to measuring x∗. This equivalence gives the\n",
      "expression\n",
      "PESn(x) = ESn(x) = H(Pn(f(x))) −Ex∗[H(Pn(f(x)|x∗))]\n",
      "(12)\n",
      "Here, the subscript in the expectation in the second term indicates that the expectation is taken over x∗.\n",
      "Unlike ES, the ﬁrst term in the PES acquisition function, H(Pn(f(x))), can be computed in closed\n",
      "form. The second term must still be approximated: Hern´andez-Lobato et al. (2014) provides a method\n",
      "for sampling x∗from the posterior distribution, and a method for approximating H(Pn(f(x)|x∗)) using\n",
      "expectation propagation (Minka, 2001). This evaluation method may then be optimized by a method\n",
      "for derivative-free optimization via simulation.\n",
      "11\n",
      "\n",
      "4.4\n",
      "Multi-Step Optimal Acquisition Functions\n",
      "We can view the act of solving problem (1) as a sequential decision-making problem (Ginsbourger and\n",
      "Riche, 2010; Frazier, 2012), in which we sequentially choose xn, and observe yn = f(xn), with the choice\n",
      "of xn depending on all past observations. At the end of these observations, we then receive a reward\n",
      "that might be equal to the value of the best point observed, maxm≤N f(xm), as it was in the analysis of\n",
      "EI, or could be equal to the value f(c\n",
      "x∗) of the objective at some new point c\n",
      "x∗chosen based on these\n",
      "observations as in the analysis of KG, or it could be the entropy of the posterior distribution on x∗as in\n",
      "ES or PES.\n",
      "By construction, the EI, KG, ES, and PES acquisition functions are optimal when N = n + 1, in the\n",
      "sense of maximizing the expected reward under the posterior. However, they are no longer obviously\n",
      "optimal when N > n + 1. In principle, it is possible to compute a multi-step optimal acquisition function\n",
      "that would maximize expected reward for general N via stochastic dynamic programming (Dynkin and\n",
      "Yushkevich, 1979), but the so-called curse of dimensionality (Powell, 2007) makes it extremely challenging\n",
      "to compute this multi-step optimal acquisition function in practice.\n",
      "Nevertheless, the literature has recently begun to deploy approximate methods for computing this so-\n",
      "lution, with attempts including Lam et al. (2016); Ginsbourger and Riche (2010); Gonz´alez et al. (2016).\n",
      "These methods do not yet seem to be in a state where they can be deployed broadly for practical prob-\n",
      "lems, because the error and extra cost introduced in solving stochastic dynamic programming problems\n",
      "approximately often overwhelms the beneﬁt that considering multiple steps provides. Nevertheless, given\n",
      "concurrent advances in reinforcement learning and approximate dynamic programming, this represents\n",
      "a promising and exciting direction for Bayesian optimization.\n",
      "In addition, there are other problem settings closely related to the one most commonly considered\n",
      "by Bayesian optimization where it is possible to compute multi-step optimal algorithms. For example,\n",
      "Cashore et al. (2016) and Xie and Frazier (2013) use problem structure to eﬃciently compute multi-step\n",
      "optimal algorithms for certain classes of Bayesian feasibility determination problems, where we wish\n",
      "to sample eﬃciently to determine whether f(x) is above or below a threshold for each x.\n",
      "Similarly,\n",
      "Waeber et al. (2013), building on Jedynak et al. (2012), computes the multi-step optimal algorithm for a\n",
      "one-dimensional stochastic root-ﬁnding problem with an entropy objective. While these optimal multi-\n",
      "step methods are only directly applicable to very speciﬁc settings, they oﬀer an opportunity to study the\n",
      "improvement possible more generally by going from one-step optimal to multi-step optimal. Surprisingly,\n",
      "in these settings, existing acquisition functions perform almost as well as the multi-step optimal algorithm.\n",
      "For example, experiments conducted in Cashore et al. (2016) show the KG acquisition function is within\n",
      "98% of optimal in the problems computed there, and Waeber et al. (2013) shows that the entropy search\n",
      "acquisition function is multi-step optimal in the setting considered there. Generalizing from these results,\n",
      "it could be that the one-step acquisition functions are close enough to optimal that further improvement\n",
      "is not practically meaningful, or it could be that multi-step optimal algorithms will provide substantially\n",
      "better performance in yet-to-be-identiﬁed practically important settings.\n",
      "5\n",
      "Exotic Bayesian Optimization\n",
      "Above we described methodology for solving the “standard” Bayesian optimization problem described\n",
      "in Section 1. This problem assumed a feasible set in which membership is easy to evaluate, such as a\n",
      "hyperrectangle or simplex; a lack of derivative information; and noise-free evaluations.\n",
      "While there are quite a few applied problems that meet all of the assumptions of the standard problem,\n",
      "there are even more where one or more of these assumptions are broken. We call these “exotic” problems.\n",
      "Here, we describe some prominent examples and give references for more detailed reading. (Although we\n",
      "discuss noisy evaluations in this section on exotic problems, they are substantially less exotic than the\n",
      "others considered, and are often considered to be part of the standard problem.)\n",
      "Noisy Evaluations\n",
      "GP regression can be extended naturally to observations with independent nor-\n",
      "mally distributed noise of known variance (Rasmussen and Williams, 2006). This adds a diagonal term\n",
      "with entries equal to the variance of the noise to the covariance matrices in (3). In practice, this variance\n",
      "is not known, and so the most common approach is to assume that the noise is of common variance and\n",
      "to include this variance as a hyperparameter. It is also possible to perform inference assuming that the\n",
      "variance changes with the domain, by modeling the log of the variance with a second Gaussian process\n",
      "(Kersting et al., 2007).\n",
      "The KG, ES, and PES acquisition functions apply directly in the setting with noise and they retain\n",
      "12\n",
      "\n",
      "their one-step optimality properties. One simply uses the posterior mean of the Gaussian process that\n",
      "includes noise.\n",
      "Direct use of the EI acquisition function presents conceptual challenges, however, since the “improve-\n",
      "ment” that results from a function value is no longer easily deﬁned, and f(x) in (7) is no longer observed.\n",
      "Authors have employed a variety of heuristic approaches, substituting diﬀerent normal distributions for\n",
      "the distribution of f(x) in (7), and typically using the maximum of the posterior mean at the previously\n",
      "evaluated points in place of f ∗\n",
      "n. Popular substitutes for the distribution of f(x) include the distribution\n",
      "of µn+1(x), the distribution of yn+1, and continuing to use the distribution of f(x) even though it is\n",
      "not observed. Because of these approximations, KG can outperform EI substantially in problems with\n",
      "substantial noise (Wu and Frazier, 2016; Frazier et al., 2009).\n",
      "As an alternative approach to applying EI when measurements are noisy, Scott et al. (2011) considers\n",
      "noisy evaluations under the restriction made in the derivation of EI: that the reported solution needs to\n",
      "be a previously reported point. It then ﬁnds the one-step optimal place to sample under this assumption.\n",
      "Its analysis is similar to that used to derive the KG policy, except that we restrict c\n",
      "x∗to those points\n",
      "that have been evaluated.\n",
      "Indeed, if we were to report a ﬁnal solution after n measurements, it would be the point among x1:n\n",
      "with the largest value of µn(x), and it would have conditional expected value µ∗∗\n",
      "n = maxi=1,...,n µn(xi).\n",
      "If we were to take one more sample at xn+1 = x, it would have conditional expected value under the\n",
      "new posterior of µ∗∗\n",
      "n+1 = maxi=1,...,n+1 µn+1(xi). Taking the expected value of the diﬀerence, the value\n",
      "of sampling at x is\n",
      "En [µ∗∗\n",
      "n+1 −µ∗∗\n",
      "n |xn+1 = x] .\n",
      "(13)\n",
      "Unlike the case with noise-free evaluations, this sample may cause µn+1(xi) to diﬀer from µn(xi) for\n",
      "i ≤n, necessitating a more complex calculation than in the noise-free setting (but a simpler calculation\n",
      "than for the KG policy). A procedure for calculating this quantity and its derivative is given in Scott\n",
      "et al. (2011). While we can view this acquisition function as an approximation to the KG acquisition\n",
      "function as Scott et al. (2011) does (they call it the KGCP acquisition function), we argue here that it\n",
      "is the most natural generalization of EI’s assumptions to the case with noisy measurements.\n",
      "Parallel Evaluations\n",
      "Performing evaluations in parallel using multiple computing resources allow\n",
      "obtaining multiple function evaluations in the time that would ordinarily be required to obtain just one\n",
      "with sequential evaluations. For this reason, parallel function evaluations is a conceptually appealing way\n",
      "to solve optimization problems in less time. EI, KG, ES, and PES can all be extended in a straightforward\n",
      "way to allow parallel function evaluations. For example, EI becomes\n",
      "EIn(x(1:q)) = En\n",
      "\u0014\n",
      "[ max\n",
      "i=1,...,q f(x(i)) −f ∗\n",
      "n]+\n",
      "\u0015\n",
      ",\n",
      "(14)\n",
      "where x(1:q) = (x(1), . . . , x(q)) is a collection of points at which we are proposing to evaluate (Ginsbourger\n",
      "et al., 2007). Parallel EI (also called multipoints EI by Ginsbourger et al. (2007)) then proposes to evaluate\n",
      "the set of points that jointly maximize this criteria. This approach can also be used asynchronously, where\n",
      "we hold ﬁxed those x(i) currently being evaluated and we allocate our idle computational resources by\n",
      "optimizing over their corresponding x(j).\n",
      "Parallel EI (14) and other parallel acquisition functions are more challenging to optimize than their\n",
      "original sequential versions from Section 4. One innovation is the Constant Liar approximation to the\n",
      "parallel EI acquisition function (Ginsbourger et al., 2010), which chooses x(i) sequentially by assuming\n",
      "that f(x(j)) for j < i have been already observed, and have values equal to a constant (usually the\n",
      "expected value of f(x(j))) under the posterior. This substantially speeds up computation. Expanding on\n",
      "this, Wang et al. (2016a) showed that inﬁnitesimal perturbation analysis can produce random stochastic\n",
      "gradients that are unbiased estimates of ∇EIn(x(1:q)), which can then be used in multistart stochastic\n",
      "gradient ascent to optimize (14). This method has been used to implement the parallel EI procedure\n",
      "for as many as q = 128 parallel evaluations. Computational methods for parallel KG were developed\n",
      "by Wu and Frazier (2016), and are implemented in the Cornell MOE software package discussed in\n",
      "Section 6. That article follows the stochastic gradient ascent approach described above in Section 4.2,\n",
      "which generalizes well to the parallel setting.\n",
      "13\n",
      "\n",
      "Constraints\n",
      "In the problem posed in Section 1, we assumed that the feasible set was a simple one in\n",
      "which it was easy to assess membership. The literature has also considered the more general problem,\n",
      "max\n",
      "x\n",
      "f(x)\n",
      "subject to gi(x) ≥0,\n",
      "i = 1, . . . , I,\n",
      "where the gi are as expensive to evaluate as f. EI generalizes naturally to this setting when f and gi\n",
      "can be evaluated without noise: improvement results when the evaluated x is feasible (gi(x) ≥0 for all\n",
      "x) and f(x) is better than the best previously evaluated feasible point. This was proposed in Section 4\n",
      "of Schonlau et al. (1998) and studied independently by Gardner et al. (2014). PES has also been studied\n",
      "for this setting (Hern´andez-Lobato et al., 2015).\n",
      "Multi-Fidelity and Multi-Information Source Evaluations\n",
      "In multi-ﬁdelity optimization,\n",
      "rather than a single objective f, we have a collection of information sources f(x, s) indexed by s. Here,\n",
      "s controls the “ﬁdelity”, with lower s giving higher ﬁdelity, and f(x, 0) corresponding to the original\n",
      "objective. Increasing the ﬁdelity (decreasing s) gives a more accurate estimate of f(x, 0), but at a higher\n",
      "cost c(s). For example, x might describe the design of an engineering system, and s the size of a mesh\n",
      "used in solving a partial diﬀerential equation that models the system. Or, s might describe the time\n",
      "horizon used in a steady-state simulation. Authors have also recently considered optimization of neural\n",
      "networks, where s indexes the number of iterations or amount of data used in training a machine learning\n",
      "algorithm (Swersky et al., 2014; Klein et al., 2016). Accuracy is modeled by supposing that f(x, s) is\n",
      "equal to f(x, 0) and is observed with noise whose variance λ(s) increases with s, or by supposing that\n",
      "f(x, s) provides deterministic evaluations with f(x, s+1)−f(x, s) modeled by a mean 0 Gaussian process\n",
      "that varies with x. Both settings can be modeled via a Gaussian process on f, including both x and s\n",
      "as part of the modeled domain.\n",
      "The overarching goal is to solve maxx f(x, 0) by observing f(x, s) at a sequence of points and ﬁdelities\n",
      "(xn, sn) with total cost PN\n",
      "n=1 c(sn) less than some budget B. Work on multi-ﬁdelity optimization includes\n",
      "Huang et al. (2006); S´obester et al. (2004); Forrester et al. (2007); McLeod et al. (2017); Kandasamy\n",
      "et al. (2016).\n",
      "In the more general problem of multi-information source optimization, we relax the assumption that\n",
      "the f(·, s) are ordered by s in terms of accuracy and cost. Instead, we simply have a function f taking a\n",
      "design input x and an information source input s, with f(x, 0) being the objective, and f(x, s) for s ̸= 0\n",
      "being observable with diﬀerent biases relative to the objective, diﬀerent amounts of noise, and diﬀerent\n",
      "costs.\n",
      "For example, x might represent the design of an aircraft’s wing, f(x, 0) the predicted performance of\n",
      "the wing under an accurate but slow simulator, and f(x, s) for s = 1, 2 representing predicted performance\n",
      "under two inexpensive approximate simulators making diﬀerent assumptions. It may be that f(x, 1) is\n",
      "accurate for some regions of the search space and substantially biased in others, with f(x, 2) being\n",
      "accurate in other regions. In this setting, the relative accuracy of f(x, 1) vs. f(x, 2) depends on x. Work\n",
      "on multi-information source optimization includes Lam et al. (2015); Poloczek et al. (2017).\n",
      "EI is diﬃcult to apply directly in these problems because evaluating f(x, s) for s ̸= 0 never provides an\n",
      "improvement in the best objective function value seen, max{f(xn, 0) : sn = 0}. Thus, a direct translation\n",
      "of EI to this setting causes EI = 0 for s ̸= 0, leading to measurement of only the highest ﬁdelity. For\n",
      "this reason, the EI-based method from Lam et al. (2015) uses EI to select xn assuming that f(x, 0) will\n",
      "be observed (even if it will not), and uses a separate procedure to select s. KG, ES, and PES can be\n",
      "applied directly to these problems, as in Poloczek et al. (2017).\n",
      "Random Environmental Conditions and Multi-Task Bayesian Optimization\n",
      "Closely\n",
      "related to multi-information source optimization is the pair of problems\n",
      "max\n",
      "x\n",
      "Z\n",
      "f(x, w)p(w) dw,\n",
      "max\n",
      "x\n",
      "X\n",
      "w\n",
      "f(x, w)p(w),\n",
      "where f is expensive to evaluate.\n",
      "These problems appear in the literature with a variety of names:\n",
      "optimization with random environmental conditions (Chang et al., 2001) in statistics, multi-task Bayesian\n",
      "optimization (Swersky et al., 2013) in machine learning, along with optimization of integrated response\n",
      "functions (Williams et al., 2000) and optimization with expensive integrands (Toscano-Palmerin and\n",
      "Frazier, 2018).\n",
      "14\n",
      "\n",
      "Rather than taking the objective\n",
      "R\n",
      "f(x, w)p(w) dw as our unit of evaluation, a natural approach is\n",
      "to evaluate f(x, w) at a small number of w at an x of interest. This gives partial information about\n",
      "the objective at x. Based on this information, one can explore a diﬀerent x, or resolve the current x\n",
      "with more precision. Moreover, by leveraging observations at w for a nearby x, one may already have\n",
      "substantial information about a particular f(x, w) reducing the need to evaluate it. Methods that act\n",
      "on this intuition can substantially outperform methods that simply evaluate the full objective in each\n",
      "evaluation via numerical quadrature or a full sum (Toscano-Palmerin and Frazier, 2018).\n",
      "This pair of problems arise in the design of engineering systems and biomedical problems, such as joint\n",
      "replacements (Chang et al., 2001) and cardiovascular bypass grafts (Xie et al., 2012), where f(x, w) is the\n",
      "performance of design x under environmental condition w as evaluated by some expensive-to-evaluate\n",
      "computational model, p(w) is some simple function (e.g., the normal density) describing the frequency\n",
      "with which condition w occurs, and our goal is to optimize average performance. It also arises in machine\n",
      "learning, in optimizing cross-validation performance. Here, we divide our data into chunks or “folds”\n",
      "indexed by w, and f(x, w) is the test performance on fold w of a machine learning model trained without\n",
      "data from this fold.\n",
      "Methods in this area include KG for noise-free (Xie et al., 2012) and general problems (Toscano-\n",
      "Palmerin and Frazier, 2018), PES (Swersky et al., 2013), and modiﬁcations of EI (Groot et al., 2010;\n",
      "Williams et al., 2000).\n",
      "As in multi-information source optimization, the unmodiﬁed EI acquisition\n",
      "function is inappropriate here because observing f(x, w) does not provide an observation of the objective\n",
      "(unless all w′ ̸= w have already been observed at that x) nor a strictly positive improvement. Thus,\n",
      "Groot et al. (2010) and Williams et al. (2000) use EI to choose x as if it we did observe the objective,\n",
      "and then use a separate strategy for choosing w.\n",
      "Derivative Observations\n",
      "Finally, we discuss optimization with derivatives. Observations of ∇f(x),\n",
      "optionally with normally distributed noise, may be incorporated directly into GP regression (Rasmussen\n",
      "and Williams, 2006, Sect 9.4). Lizotte (2008) proposed using gradient information in this way in Bayesian\n",
      "optimization, together with the EI acquisition function, showing an improvement over BFGS (Liu and\n",
      "Nocedal, 1989). EI is unchanged by a proposed observation of ∇f(x) in addition to f(x) as compared to\n",
      "its value when observing f(x) alone. (Though, if previous derivative observations have contributed to the\n",
      "time n posterior, then that time-n posterior will diﬀer from what it would be if we had observed only f(x).)\n",
      "Thus, EI does not take advantage of the availability of derivative information to, for example, evaluate\n",
      "at points far away from previously evaluated ones where derivative information would be particularly\n",
      "useful. A KG method alleviating this problem was proposed by Wu et al. (2017). In other related work\n",
      "in this area, Osborne et al. (2009) proposed using gradient information to improve conditioning of the\n",
      "covariance matrix in GP regression, and Ahmed et al. (2016) proposed a method for choosing a single\n",
      "directional derivative to retain when observing gradients to improve the computational tractability of\n",
      "GP inference.\n",
      "6\n",
      "Software\n",
      "There are a variety of codes for Bayesian optimization and Gaussian process regression.\n",
      "Several of\n",
      "these Gaussian process regression and Bayesian optimization packages are developed together, with the\n",
      "Bayesian optimization package making use of the Gaussian process regression package. Other packages are\n",
      "standalone, providing only either Gaussian process regression support or Bayesian optimization support.\n",
      "We list here several of the most prominent packages, along with URLs that are current as of June 2018.\n",
      "• DiceKriging and DiceOptim are packages for Gaussian process regression and Bayesian optimization\n",
      "respectively, written in R. They are described in detail in Roustant et al. (2012) and are available\n",
      "from CRAN via https://cran.r-project.org/web/packages/DiceOptim/index.html.\n",
      "• GPyOpt (https://github.com/SheffieldML/GPyOpt) is a python Bayesian optimization library\n",
      "built on top of the Gaussian process regression library GPy (https://sheffieldml.github.io/\n",
      "GPy/) both written and maintained by the machine learning group at Sheﬃeld University.\n",
      "• Metrics Optimization Engine (MOE, https://github.com/Yelp/MOE) is a Bayesian optimization\n",
      "library in C++ with a python wrapper that supports GPU-based computations for improved speed.\n",
      "It was developed at Yelp by the founders of the Bayesian optimization startup, SigOpt (http://\n",
      "sigopt.com). Cornell MOE (https://github.com/wujian16/Cornell-MOE) is built on MOE with\n",
      "changes that make it easier to install, and support for parallel and derivative-enabled knowledge-\n",
      "gradient algorithms.\n",
      "15\n",
      "\n",
      "• Spearmint (https://github.com/HIPS/Spearmint), with an older version under a diﬀerent license\n",
      "available at https://github.com/JasperSnoek/spearmint, is a python Bayesian optimization li-\n",
      "brary. Spearmint was written by the founders of the Bayesian optimization startup Whetlab, which\n",
      "was acquired by Twitter in 2015 Perez (2015).\n",
      "• DACE (Design and Analysis of Computer Experiments) is a Gaussian process regression library\n",
      "written in MATLAB, available at http://www2.imm.dtu.dk/projects/dace/. Although it was last\n",
      "updated in 2002, it remains widely used.\n",
      "• GPFlow (https://github.com/GPflow/GPflow) and GPyTorch (https://github.com/cornellius-gp/\n",
      "gpytorch) are python Gaussian process regression library built on top of Tensorﬂow (https:\n",
      "//www.tensorflow.org/) and PyTorch (https://pytorch.org/) respectively.\n",
      "• laGP (https://cran.r-project.org/web/packages/laGP/index.html) is an R package for Gaus-\n",
      "sian process regression and Bayesian optimization with support for inequality constraints.\n",
      "7\n",
      "Conclusion and Research Directions\n",
      "We have introduced Bayesian optimization, ﬁrst discussing GP regression, then the expected improve-\n",
      "ment, knowledge gradient, entropy search, and predictive entropy search acquisition functions. We then\n",
      "discussed a variety of exotic Bayesian optimization problems: those with noisy measurements; paral-\n",
      "lel evaluations; constraints; multiple ﬁdelities and multiple information sources; random environmental\n",
      "conditions and multi-task BayesOpt; and derivative observations.\n",
      "Many research directions present themselves in this exciting ﬁeld. First, there is substantial room\n",
      "for developing a deeper theoretical understanding of Bayesian optimization. As described in Section 4.4,\n",
      "settings where we can compute multi-step optimal algorithms are extremely limited. Moreover, while\n",
      "the acquisition functions we currently use in practice seem to perform almost as well as optimal multi-\n",
      "step algorithms when we can compute them, we do not currently have ﬁnite-time bounds that explain\n",
      "their near-optimal empirical performance, nor do we know whether multi-step optimal algorithms can\n",
      "provide substantial practical beneﬁt in yet-to-be-understood settings. Even in the asymptotic regime,\n",
      "relatively little is known about rates of convergence for Bayesian optimization algorithms: while Bull\n",
      "(2011) establishes a rate of convergence for expected improvement when it is combined with periodic\n",
      "uniform sampling, it is unknown whether removing uniform sampling results in the same or diﬀerent\n",
      "rate.\n",
      "Second, there is room to build Bayesian optimization methods that leverage novel statistical ap-\n",
      "proaches. Gaussian processes (or variants thereof such as Snoek et al. (2014) and Kersting et al. (2007))\n",
      "are used in most work on Bayesian optimization, but it seems likely that classes of problems exist where\n",
      "the objective could be better modeled through other approaches. It is both of interest to develop new\n",
      "statistical models that are broadly useful, and to develop models that are speciﬁcally designed for appli-\n",
      "cations of interest.\n",
      "Third, developing Bayesian optimization methods that work well in high dimensions is of great prac-\n",
      "tical and theoretical interest. Directions for research include developing statistical methods that identify\n",
      "and leverage structure present in high-dimensional objectives arising in practice, which has been pur-\n",
      "sued by recent work including Wang et al. (2013, 2016b); Kandasamy et al. (2015). See also Shan and\n",
      "Wang (2010). It is also possible that new acquisition functions may provide substantial value in high\n",
      "dimensional problems.\n",
      "Fourth, it is of interest to develop methods that leverage exotic problem structure unconsidered\n",
      "by today’s methods, in the spirit of the Section 5.\n",
      "It may be particularly fruitful to combine such\n",
      "methodological development with applying Bayesian optimization to important real-world problems, as\n",
      "using methods in the real world tends to reveal unanticipated diﬃculties and spur creativity.\n",
      "Fifth, substantial impact in a variety of ﬁelds seems possible through application of Bayesian opti-\n",
      "mization. One set of application areas where Bayesian optimization seems particularly well-positioned\n",
      "to oﬀer impact is in chemistry, chemical engineering, materials design, and drug discovery, where prac-\n",
      "titioners undertake design eﬀorts involving repeated physical experiments consuming years of eﬀort and\n",
      "substantial monetary expense. While there is some early work in these areas (Ueno et al., 2016; Frazier\n",
      "and Wang, 2016; Negoescu et al., 2011; Seko et al., 2015; Ju et al., 2017) the number of researchers\n",
      "working in these ﬁelds aware of the power and applicability of Bayesian optimization is still relatively\n",
      "small.\n",
      "16\n",
      "\n",
      "Acknowledgments\n",
      "While writing this tutorial, the author was supported by the Air Force Oﬃce of Scientiﬁc Research and\n",
      "the National Science Foundation (AFOSR FA9550-15-1-0038 and NSF CMMI-1254298, CMMI-1536895\n",
      "DMR-1719875, DMR-1120296). The author would also like to thank Roman Garnett, whose suggestions\n",
      "helped shape the discussion of expected improvement with noise, and several anonymous reviewers.\n",
      "References\n",
      "Ahmed, M. O., Shahriari, B., and Schmidt, M. (2016). Do we need “harmless” Bayesian optimization\n",
      "and “ﬁrst-order” Bayesian optimization. In Neural Information Processing Systems 2016 Workshop\n",
      "on Bayesian Optimization.\n",
      "Berger, J. O. (2013). Statistical Decision Theory and Bayesian Analysis. Springer Science & Business\n",
      "Media.\n",
      "Blum, J. R. (1954). Multidimensional stochastic approximation methods. The Annals of Mathematical\n",
      "Statistics, pages 737–744.\n",
      "Booker, A., Dennis, J., Frank, P., Seraﬁni, D., Torczon, V., and Trosset, M. (1999). A rigorous framework\n",
      "for optimization of expensive functions by surrogates. Structural and Multidisciplinary Optimization,\n",
      "17(1):1–13.\n",
      "Bottou, L. (2012). Stochastic gradient descent tricks. In Montavon, G., Orr, G. B., and M¨uller, K. R.,\n",
      "editors, Neural Networks: Tricks of the Trade, pages 421–436. Springer.\n",
      "Brochu, E., Cora, M., and de Freitas, N. (2009).\n",
      "A tutorial on bayesian optimization of expensive\n",
      "cost functions, with application to active user modeling and hierarchical reinforcement learning.\n",
      "Technical Report TR-2009-023, Department of Computer Science, University of British Columbia.\n",
      "arXiv:1012.2599.\n",
      "Bull, A. D. (2011). Convergence rates of eﬃcient global optimization algorithms. Journal of Machine\n",
      "Learning Research, 12(Oct):2879–2904.\n",
      "Calvin, J. (1997). Average performance of a class of adaptive algorithms for global optimization. The\n",
      "Annals of Applied Probability, 7(3):711–730.\n",
      "Calvin, J. and ˇZilinskas, A. (2005). One-dimensional global optimization for observations with noise.\n",
      "Computers & Mathematics with Applications, 50(1-2):157–169.\n",
      "Calvin, J. and ˇZilinskas, A. (1999). On the convergence of the P-algorithm for one-dimensional global\n",
      "optimization of smooth functions. Journal of Optimization Theory and Applications, 102(3):479–495.\n",
      "Calvin, J. and ˇZilinskas, A. (2000). One-dimensional P-algorithm with convergence rate O(n-3+δ) for\n",
      "smooth functions. Journal of Optimization Theory and Applications, 106(2):297–307.\n",
      "Cashore, J. M., Kumarga, L., and Frazier, P. I. (2016).\n",
      "Multi-step Bayesian optimization for one-\n",
      "dimensional feasibility determination. arXiv preprint arXiv:1607.03195.\n",
      "Chang, P. B., Williams, B. J., Bhalla, K. S. B., Belknap, T. W., Santner, T. J., Notz, W. I., and Bartel,\n",
      "D. L. (2001). Design and analysis of robust total joint replacements: ﬁnite element model experiments\n",
      "with environmental variables. Journal of Biomechanical Engineering, 123(3):239–246.\n",
      "Chick, S. E. and Inoue, K. (2001).\n",
      "New two-stage and sequential procedures for selecting the best\n",
      "simulated system. Operations Research, 49(5):732–743.\n",
      "Clark, C. E. (1961). The greatest of a ﬁnite set of random variables. Operations Research, 9(2):145–162.\n",
      "Cover, T. M. and Thomas, J. A. (2012). Elements of Information Theory. John Wiley & Sons.\n",
      "Dynkin, E. and Yushkevich, A. (1979). Controlled Markov Processes. Springer, New York.\n",
      "Forrester, A., S´obester, A., and Keane, A. (2008).\n",
      "Engineering Design via Surrogate Modelling: A\n",
      "Practical Guide. Wiley, West Sussex, UK.\n",
      "17\n",
      "\n",
      "Forrester, A. I., S´obester, A., and Keane, A. J. (2007). Multi-ﬁdelity optimization via surrogate modelling.\n",
      "In Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences,\n",
      "volume 463, pages 3251–3269. The Royal Society.\n",
      "Frazier, P., Powell, W., and Dayanik, S. (2009). The knowledge-gradient policy for correlated normal\n",
      "beliefs. INFORMS Journal on Computing, 21(4):599–613.\n",
      "Frazier, P. I. (2012). Tutorial: Optimization via simulation with bayesian statistics and dynamic pro-\n",
      "gramming. In Laroque, C., Himmelspach, J., Pasupathy, R., Rose, O., and Uhrmacher, A. M., editors,\n",
      "Proceedings of the 2012 Winter Simulation Conference Proceedings, pages 79–94, Piscataway, New\n",
      "Jersey. Institute of Electrical and Electronics Engineers, Inc.\n",
      "Frazier, P. I., Powell, W. B., and Dayanik, S. (2008). A knowledge-gradient policy for sequential infor-\n",
      "mation collection. SIAM Journal on Control and Optimization, 47(5):2410–2439.\n",
      "Frazier, P. I. and Wang, J. (2016). Bayesian optimization for materials design. In Lookman, T., Alexander,\n",
      "F. J., and Rajan, K., editors, Information Science for Materials Discovery and Design, pages 45–75.\n",
      "Springer.\n",
      "Gardner, J. R., Kusner, M. J., Xu, Z. E., Weinberger, K. Q., and Cunningham, J. P. (2014). Bayesian\n",
      "optimization with inequality constraints. In ICML, pages 937–945.\n",
      "Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2014). Bayesian\n",
      "Data Analysis, volume 2. CRC Press Boca Raton, FL.\n",
      "Ginsbourger, D., Le Riche, R., and Carraro, L. (2007). A multi-points criterion for deterministic par-\n",
      "allel global optimization based on kriging. In International Conference on Nonconvex Programming,\n",
      "NCP07, Rouen, France.\n",
      "Ginsbourger, D., Le Riche, R., and Carraro, L. (2010). Kriging is well-suited to parallelize optimization.\n",
      "In Tenne, Y. and Goh, C. K., editors, Computational Intelligence in Expensive Optimization Problems,\n",
      "volume 2, pages 131–162. Springer.\n",
      "Ginsbourger, D. and Riche, R. (2010). Towards Gaussian process-based optimization with ﬁnite time\n",
      "horizon. In Giovagnoli, A., Atkinson, A., Torsney, B., and May, C., editors, mODa 9–Advances in\n",
      "Model-Oriented Design and Analysis, pages 89–96. Springer.\n",
      "Gonz´alez, J., Osborne, M., and Lawrence, N. (2016).\n",
      "GLASSES: Relieving the myopia of bayesian\n",
      "optimisation. In Artiﬁcial Intelligence and Statistics, pages 790–799.\n",
      "Groot, P., Birlutiu, A., and Heskes, T. (2010).\n",
      "Bayesian monte carlo for the global optimization of\n",
      "expensive functions. In ECAI, pages 249–254.\n",
      "Hennig, P. and Schuler, C. J. (2012). Entropy search for information-eﬃcient global optimization. Journal\n",
      "of Machine Learning Research, 13:1809–1837.\n",
      "Hern´andez-Lobato, J. M., Gelbart, M. A., Hoﬀman, M. W., Adams, R. P., and Ghahramani, Z. (2015).\n",
      "Predictive entropy search for bayesian optimization with unknown constraints. In Proceedings of the\n",
      "32nd International Conference on International Conference on Machine Learning-Volume 37, pages\n",
      "1699–1707. JMLR. org.\n",
      "Hern´andez-Lobato, J. M., Hoﬀman, M. W., and Ghahramani, Z. (2014). Predictive entropy search for\n",
      "eﬃcient global optimization of black-box functions.\n",
      "In Advances in neural information processing\n",
      "systems, pages 918–926.\n",
      "Ho, Y.-C., Cao, X., and Cassandras, C. (1983). Inﬁnitesimal and ﬁnite perturbation analysis for queueing\n",
      "networks. Automatica, 19(4):439–445.\n",
      "Huang, D., Allen, T., Notz, W., and Miller, R. (2006). Sequential kriging optimization using multiple-\n",
      "ﬁdelity evaluations. Structural and Multidisciplinary Optimization, 32(5):369–382.\n",
      "Jedynak, B., Frazier, P. I., and Sznitman, R. (2012). Twenty questions with noise: Bayes optimal policies\n",
      "for entropy loss. Journal of Applied Probability, 49(1):114–136.\n",
      "18\n",
      "\n",
      "Jones, D. R., Schonlau, M., and Welch, W. J. (1998). Eﬃcient global optimization of expensive black-box\n",
      "functions. Journal of Global Optimization, 13(4):455–492.\n",
      "Ju, S., Shiga, T., Feng, L., Hou, Z., Tsuda, K., and Shiomi, J. (2017). Designing nanostructures for\n",
      "phonon transport via Bayesian optimization. Physical Review X, 7.\n",
      "Kaelbling, L. P., Littman, M. L., and Moore, A. W. (1996). Reinforcement learning: A survey. Journal\n",
      "of Artiﬁcial Intelligence Research, 4:237–285.\n",
      "Kandasamy, K., Dasarathy, G., Oliva, J. B., Schneider, J., and P´oczos, B. (2016). Gaussian process\n",
      "bandit optimisation with multi-ﬁdelity evaluations.\n",
      "In Advances in Neural Information Processing\n",
      "Systems, pages 992–1000.\n",
      "Kandasamy, K., Schneider, J., and P´oczos, B. (2015).\n",
      "High dimensional bayesian optimisation and\n",
      "bandits via additive models. In International Conference on Machine Learning, pages 295–304.\n",
      "Keane, A. (2006). Statistical improvement criteria for use in multiobjective design optimization. AIAA\n",
      "Journal, 44(4):879–891.\n",
      "Kersting, K., Plagemann, C., Pfaﬀ, P., and Burgard, W. (2007). Most likely heteroscedastic gaussian\n",
      "process regression. In Proceedings of the 24th International Conference on Machine learning, pages\n",
      "393–400. ACM.\n",
      "Kleijnen, J. P. et al. (2008). Design and Analysis of Simulation Experiments, volume 20. Springer.\n",
      "Klein, A., Falkner, S., Bartels, S., Hennig, P., and Hutter, F. (2016). Fast Bayesian optimization of\n",
      "machine learning hyperparameters on large datasets. arXiv preprint arXiv:1605.07079.\n",
      "Knowles, J. (2006). ParEGO: A hybrid algorithm with on-line landscape approximation for expensive\n",
      "multiobjective optimization problems. IEEE Transactions on Evolutionary Computation, 10(1):50–66.\n",
      "Kushner, H. J. (1964). A new method of locating the maximum point of an arbitrary multipeak curve\n",
      "in the presence of noise. Journal of Basic Engineering, 86(1):97–106.\n",
      "Lam, R., Allaire, D. L., and Willcox, K. E. (2015). Multiﬁdelity optimization using statistical surro-\n",
      "gate modeling for non-hierarchical information sources. In 56th AIAA/ASCE/AHS/ASC Structures,\n",
      "Structural Dynamics, and Materials Conference, page 0143.\n",
      "Lam, R., Willcox, K., and Wolpert, D. H. (2016).\n",
      "Bayesian optimization with a ﬁnite budget: An\n",
      "approximate dynamic programming approach. In Advances in Neural Information Processing Systems,\n",
      "pages 883–891.\n",
      "Liu, D. C. and Nocedal, J. (1989). On the limited memory BFGS method for large scale optimization.\n",
      "Mathematical Programming, 45(1-3):503–528.\n",
      "Lizotte, D. (2008). Practical Bayesian Optimization. PhD thesis, University of Alberta.\n",
      "Lizotte, D., Wang, T., Bowling, M., and Schuurmans, D. (2007).\n",
      "Automatic gait optimization with\n",
      "Gaussian process regression. In Proceedings of IJCAI, pages 944–949.\n",
      "Mahajan, A. and Teneketzis, D. (2008).\n",
      "Multi-armed bandit problems.\n",
      "In Hero, A., Casta˜n´on, D.,\n",
      "Cochran, D., and Kastella, K., editors, Foundations and Applications of Sensor Management, pages\n",
      "121–151. Springer.\n",
      "Mart´ı, R., Lozano, J. A., Mendiburu, A., and Hernando, L. (2016). Multi-start methods. Handbook of\n",
      "Heuristics, pages 1–21.\n",
      "McLeod, M., Osborne, M. A., and Roberts, S. J. (2017). Practical bayesian optimization for variable\n",
      "cost objectives. arXiv preprint arXiv:1703.04335.\n",
      "Mehdad, E. and Kleijnen, J. P. (2018). Eﬃcient global optimisation for black-box simulation via sequen-\n",
      "tial intrinsic kriging. Journal of the Operational Research Society, 69:1–13.\n",
      "Milgrom, P. and Segal, I. (2002). Envelope theorems for arbitrary choice sets. Econometrica, 70(2):583–\n",
      "601.\n",
      "19\n",
      "\n",
      "Minka, T. P. (2001).\n",
      "A family of algorithms for approximate Bayesian inference.\n",
      "PhD thesis, Mas-\n",
      "sachusetts Institute of Technology.\n",
      "Moˇckus, J. (1975). On Bayesian methods for seeking the extremum. In Optimization Techniques IFIP\n",
      "Technical Conference, pages 400–404. Springer.\n",
      "Moˇckus, J. (1989). Bayesian Approach to Global Optimization: Theory and Applications. Kluwer Aca-\n",
      "demic Publishers.\n",
      "Moˇckus, J. and Moˇckus, L. (1991). Bayesian approach to global optimization and application to multi-\n",
      "objective and constrained problems. Journal of Optimization Theory and Applications, 70(1):157–172.\n",
      "Moˇckus, J., Tiesis, V., and ˇZilinskas, A. (1978). The application of Bayesian methods for seeking the\n",
      "extremum. In Dixon, L. and Szego, G., editors, Towards Global Optimisation, volume 2, pages 117–129.\n",
      "Elsevier Science Ltd., North Holland, Amsterdam.\n",
      "Neal, R. M. (2003). Slice sampling. Annals of Statistics, 31(3):705–741.\n",
      "Negoescu, D. M., Frazier, P. I., and Powell, W. B. (2011).\n",
      "The knowledge gradient algorithm for\n",
      "sequencing experiments in drug discovery. INFORMS Journal on Computing, 23(1):46–363.\n",
      "Osborne, M. A., Garnett, R., and Roberts, S. J. (2009). Gaussian processes for global optimization. In\n",
      "3rd International Conference on Learning and Intelligent Optimization (LION3), pages 1–15. Citeseer.\n",
      "Packwood, D. (2017). Bayesian Optimization for Materials Science, volume 3. Springer.\n",
      "Perez, S. (2015). Twitter acquires machine learning startup whetlab. TechCrunch. Accessed July 3,\n",
      "2018.\n",
      "Poloczek, M., Wang, J., and Frazier, P. (2017). Multi-information source optimization. In Advances in\n",
      "Neural Information Processing Systems, pages 4291–4301.\n",
      "Powell, W. B. (2007). Approximate Dynamic Programming: Solving the Curses of Dimensionality. John\n",
      "Wiley & Sons, New York.\n",
      "Rasmussen, C. and Williams, C. (2006). Gaussian Processes for Machine Learning. MIT Press, Cam-\n",
      "bridge, MA.\n",
      "Regis, R. and Shoemaker, C. (2005). Constrained global optimization of expensive black box functions\n",
      "using radial basis functions. Journal of Global Optimization, 31(1):153–171.\n",
      "Regis, R. and Shoemaker, C. (2007a). Improved strategies for radial basis function methods for global\n",
      "optimization. Journal of Global Optimization, 37(1):113–135.\n",
      "Regis, R. and Shoemaker, C. (2007b). Parallel radial basis function methods for the global optimization\n",
      "of expensive functions. European Journal of Operational Research, 182(2):514–535.\n",
      "Robbins, H. and Monro, S. (1951). A stochastic approximation method. The Annals of Mathematical\n",
      "Statistics, 22(3):400–407.\n",
      "Roustant, O., Ginsbourger, D., and Deville, Y. (2012).\n",
      "Dicekriging, diceoptim: Two r packages for\n",
      "the analysis of computer experiments by kriging-based metamodeling and optimization. Journal of\n",
      "Statistical Software, Articles, 51(1):1–55.\n",
      "Salemi, P., Nelson, B. L., and Staum, J. (2014). Discrete optimization via simulation using Gaussian\n",
      "Markov random ﬁelds. In Proceedings of the 2014 Winter Simulation Conference, pages 3809–3820.\n",
      "IEEE Press.\n",
      "Sasena, M. (2002). Flexibility and Eﬃciency Enhancements for Constrained Global Design Optimization\n",
      "with Kriging Approximations. PhD thesis, University of Michigan.\n",
      "Schonlau, M., Welch, W. J., and Jones, D. R. (1998). Global versus local search in constrained optimiza-\n",
      "tion of computer models. Lecture Notes — Monograph Series, 34:11–25.\n",
      "20\n",
      "\n",
      "Scott, W., Frazier, P. I., and Powell, W. B. (2011). The correlated knowledge gradient for simulation opti-\n",
      "mization of continuous parameters using Gaussian process regression. SIAM Journal on Optimization,\n",
      "21(3):996–1026.\n",
      "Seko, A., Togo, A., Hayashi, H., Tsuda, K., Chaput, L., and Tanaka, I. (2015). Prediction of low-thermal-\n",
      "conductivity compounds with ﬁrst-principles anharmonic lattice-dynamics calculations and Bayesian\n",
      "optimization. Physical Review Letters, 115.\n",
      "Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., and de Freitas, N. (2016). Taking the human out of\n",
      "the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1):148–175.\n",
      "Shan, S. and Wang, G. G. (2010).\n",
      "Survey of modeling and optimization strategies to solve high-\n",
      "dimensional design problems with computationally-expensive black-box functions.\n",
      "Structural and\n",
      "Multidisciplinary Optimization, 41(2):219–241.\n",
      "Shoemaker, C., Regis, R., and Fleming, R. (2007). Watershed calibration using multistart local opti-\n",
      "mization and evolutionary optimization with radial basis function approximation/calage au niveau du\n",
      "bassin versant `a l’aide d’une optimisation locale `a d´emarrage multiple et d’une optimisation ´evolutive\n",
      "avec approximation `a fonctions de base radiale. Hydrological Sciences Journal/Journal des Sciences\n",
      "Hydrologiques, 52(3):450–465.\n",
      "Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian optimization of machine learning\n",
      "algorithms. In Advances in Neural Information Processing Systems, pages 2951–2959.\n",
      "Snoek, J., Swersky, K., Zemel, R., and Adams, R. (2014). Input warping for Bayesian optimization of\n",
      "non-stationary functions. In International Conference on Machine Learning, pages 1674–1682.\n",
      "S´obester, A., Leary, S., and Keane, A. (2004). A parallel updating scheme for approximating and optimiz-\n",
      "ing high ﬁdelity computer simulations. Structural and Multidisciplinary Optimization, 27(5):371–383.\n",
      "Sutton, R. S. and Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT press Cambridge.\n",
      "Swersky, K., Snoek, J., and Adams, R. P. (2013). Multi-task Bayesian optimization. In Advances in\n",
      "Neural Information Processing Systems, pages 2004–2012.\n",
      "Swersky, K., Snoek, J., and Adams, R. P. (2014). Freeze-thaw bayesian optimization. arXiv preprint\n",
      "arXiv:1406.3896.\n",
      "Toscano-Palmerin, S. and Frazier, P. I. (2018). Bayesian optimization with expensive integrands. arXiv\n",
      "preprint arXiv:1803.08661.\n",
      "Ueno, T., Rhone, T. D., Hou, Z., Mizoguchi, T., and Tsuda, K. (2016). COMBO: An eﬃcient Bayesian\n",
      "optimization library for materials science. Materials Discovery, 4:18–21.\n",
      "ˇZilinskas, A. (1975). Single-step Bayesian search method for an extremum of functions of a single variable.\n",
      "Cybernetics and Systems Analysis, 11(1):160–166.\n",
      "Waeber, R., Frazier, P. I., and Henderson, S. G. (2013). Bisection search with noisy responses. SIAM\n",
      "Journal on Control and Optimization, 51(3):2261–2279.\n",
      "Wang, J., Clark, S. C., Liu, E., and Frazier, P. I. (2016a).\n",
      "Parallel Bayesian global optimization of\n",
      "expensive functions. arXiv preprint arXiv:1602.05149.\n",
      "Wang, Z., Hutter, F., Zoghi, M., Matheson, D., and de Feitas, N. (2016b). Bayesian optimization in a\n",
      "billion dimensions via random embeddings. Journal of Artiﬁcial Intelligence Research, 55:361–387.\n",
      "Wang, Z., Zoghi, M., Hutter, F., Matheson, D., De Freitas, N., et al. (2013). Bayesian optimization in\n",
      "high dimensions via random embeddings. In IJCAI, pages 1778–1784.\n",
      "Williams, B. J., Santner, T. J., and Notz, W. I. (2000). Sequential design of computer experiments to\n",
      "minimize integrated response functions. Statistica Sinica, 10(4):1133–1152.\n",
      "Wu, J. and Frazier, P. (2016). The parallel knowledge gradient method for batch Bayesian optimization.\n",
      "In Advances in Neural Information Processing Systems, pages 3126–3134.\n",
      "21\n",
      "\n",
      "Wu, J., Poloczek, M., Wilson, A. G., and Frazier, P. (2017). Bayesian optimization with gradients. In\n",
      "Advances in Neural Information Processing Systems, pages 5273–5284.\n",
      "Xie, J. and Frazier, P. I. (2013). Sequential Bayes-optimal policies for multiple comparisons with a known\n",
      "standard. Operations Research, 61(5):1174–1189.\n",
      "Xie, J., Frazier, P. I., Sankaran, S., Marsden, A., and Elmohamed, S. (2012). Optimization of compu-\n",
      "tationally expensive simulations with Gaussian processes and parameter uncertainty: Application to\n",
      "cardiovascular surgery. In 50th Annual Allerton Conference on Communication, Control, and Com-\n",
      "puting, pages 406–413. IEEE.\n",
      "22\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = r\"C:\\Users\\praye\\Downloads\\bayesian_optimization.pdf\"\n",
    "\n",
    "# Initialize an empty string to hold the extracted text\n",
    "extracted_text = \"\"\n",
    "\n",
    "# Open the PDF file\n",
    "with fitz.open(pdf_path) as pdf:\n",
    "    for page in pdf:\n",
    "        extracted_text += page.get_text() + \"\\n\"  # Append text from each page\n",
    "\n",
    "print(\"Extracted Text:\", extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2462de-7caa-48a1-add4-0e39e5e1b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0540c57d-b431-4e9e-b690-70ec2b1c06cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Bayesian optimization is an approach to optimizing objective functions that take a long time to evaluate.\n",
      "It is best-suited for optimization over continuous domains of less than 20-dimensions.\n",
      "BayesOpt has been used extensively for engineering systems since the 1960s.\n",
      "The ability to optimize expensive black box functions makes it extremely popular for testing hyperparameters in machine learning.\n",
      "We conclude with a discussion of Bayesian optimization software and future research directions in the ﬁeld.com tutorial.\n",
      "Our focus is on �nding a global rather than local optimum, our focus on the BayesOpt is on the black box.com guide to Bayes Optimization.com/Bayes Optimism.com's \"Bayesoptimism\" and our guide to the guide to more advanced techniques, which we will use in the guide.com-to-planning.com launch of the guide, which will be available on July 10, 2018.com.com: Back to Back to the page you came from the back of the Back of the Web.com page: Share your memories. Back to back to the back to your memories of the page.com. Back of your memories, back of your stories, back to back of it.com and back of them. Back of them, and back to Back of it, and your memories and back.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input\n",
    "inputs = tokenizer(extracted_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = model.generate(inputs['input_ids'], max_length=350, min_length=280, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode the summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8432d86-6253-41c3-a930-abaaa9ba39c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e69c9c8-b4b5-4d7a-bf1b-a16adeea16c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\praye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\praye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a7f3e47-f4e1-4b22-8ba2-48cc59d6c26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('Bayesian', 'JJ'), ('optimization', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('approach', 'NN'), ('to', 'TO'), ('optimizing', 'VBG'), ('objective', 'JJ'), ('functions', 'NNS'), ('that', 'WDT'), ('take', 'VBP'), ('a', 'DT'), ('long', 'JJ'), ('time', 'NN'), ('to', 'TO'), ('evaluate', 'VB'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('best-suited', 'JJ'), ('for', 'IN'), ('optimization', 'NN'), ('over', 'IN'), ('continuous', 'JJ'), ('domains', 'NNS'), ('of', 'IN'), ('less', 'JJR'), ('than', 'IN'), ('20-dimensions', 'NNS'), ('.', '.'), ('BayesOpt', 'NNP'), ('has', 'VBZ'), ('been', 'VBN'), ('used', 'VBN'), ('extensively', 'RB'), ('for', 'IN'), ('engineering', 'NN'), ('systems', 'NNS'), ('since', 'IN'), ('the', 'DT'), ('1960s', 'NNS'), ('.', '.'), ('The', 'DT'), ('ability', 'NN'), ('to', 'TO'), ('optimize', 'VB'), ('expensive', 'JJ'), ('black', 'JJ'), ('box', 'NN'), ('functions', 'NNS'), ('makes', 'VBZ'), ('it', 'PRP'), ('extremely', 'RB'), ('popular', 'JJ'), ('for', 'IN'), ('testing', 'VBG'), ('hyperparameters', 'NNS'), ('in', 'IN'), ('machine', 'NN'), ('learning', 'NN'), ('.', '.'), ('We', 'PRP'), ('conclude', 'VBP'), ('with', 'IN'), ('a', 'DT'), ('discussion', 'NN'), ('of', 'IN'), ('Bayesian', 'JJ'), ('optimization', 'NN'), ('software', 'NN'), ('and', 'CC'), ('future', 'JJ'), ('research', 'NN'), ('directions', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('ﬁeld.com', 'NNP'), ('tutorial', 'NN'), ('.', '.'), ('Our', 'PRP$'), ('focus', 'NN'), ('is', 'VBZ'), ('on', 'IN'), ('�nding', 'VBG'), ('a', 'DT'), ('global', 'JJ'), ('rather', 'RB'), ('than', 'IN'), ('local', 'JJ'), ('optimum', 'NN'), (',', ','), ('our', 'PRP$'), ('focus', 'NN'), ('on', 'IN'), ('the', 'DT'), ('BayesOpt', 'NNP'), ('is', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('black', 'JJ'), ('box.com', 'NN'), ('guide', 'NN'), ('to', 'TO'), ('Bayes', 'NNP'), ('Optimization.com/Bayes', 'NNP'), ('Optimism.com', 'NNP'), (\"'s\", 'POS'), ('``', '``'), ('Bayesoptimism', 'NNP'), (\"''\", \"''\"), ('and', 'CC'), ('our', 'PRP$'), ('guide', 'NN'), ('to', 'TO'), ('the', 'DT'), ('guide', 'NN'), ('to', 'TO'), ('more', 'RBR'), ('advanced', 'JJ'), ('techniques', 'NNS'), (',', ','), ('which', 'WDT'), ('we', 'PRP'), ('will', 'MD'), ('use', 'VB'), ('in', 'IN'), ('the', 'DT'), ('guide.com-to-planning.com', 'JJ'), ('launch', 'NN'), ('of', 'IN'), ('the', 'DT'), ('guide', 'NN'), (',', ','), ('which', 'WDT'), ('will', 'MD'), ('be', 'VB'), ('available', 'JJ'), ('on', 'IN'), ('July', 'NNP'), ('10', 'CD'), (',', ','), ('2018.com.com', 'CD'), (':', ':'), ('Back', 'NN'), ('to', 'TO'), ('Back', 'NNP'), ('to', 'TO'), ('the', 'DT'), ('page', 'NN'), ('you', 'PRP'), ('came', 'VBD'), ('from', 'IN'), ('the', 'DT'), ('back', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Back', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Web.com', 'NNP'), ('page', 'NN'), (':', ':'), ('Share', 'VB'), ('your', 'PRP$'), ('memories', 'NNS'), ('.', '.'), ('Back', 'NNP'), ('to', 'TO'), ('back', 'VB'), ('to', 'TO'), ('the', 'DT'), ('back', 'NN'), ('to', 'TO'), ('your', 'PRP$'), ('memories', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('page.com', 'NN'), ('.', '.'), ('Back', 'NNP'), ('of', 'IN'), ('your', 'PRP$'), ('memories', 'NNS'), (',', ','), ('back', 'RB'), ('of', 'IN'), ('your', 'PRP$'), ('stories', 'NNS'), (',', ','), ('back', 'RB'), ('to', 'TO'), ('back', 'NN'), ('of', 'IN'), ('it.com', 'NN'), ('and', 'CC'), ('back', 'NN'), ('of', 'IN'), ('them', 'PRP'), ('.', '.'), ('Back', 'NNP'), ('of', 'IN'), ('them', 'PRP'), (',', ','), ('and', 'CC'), ('back', 'RB'), ('to', 'TO'), ('Back', 'NNP'), ('of', 'IN'), ('it', 'PRP'), (',', ','), ('and', 'CC'), ('your', 'PRP$'), ('memories', 'NNS'), ('and', 'CC'), ('back', 'JJ'), ('stories', 'NNS'), ('.', '.'), ('See', 'VB'), ('back', 'RB'), ('to', 'TO'), ('us', 'PRP'), ('.', '.'), ('Back', 'CC'), ('at', 'IN'), ('the', 'DT'), ('bottom', 'NN'), ('of', 'IN'), ('the', 'DT'), ('story', 'NN'), ('.', '.'), ('Back', 'NNP'), ('to', 'TO'), ('your', 'PRP$'), ('stories', 'NNS'), ('..', 'VBP'), ('com', 'NN'), (',', ','), ('back', 'RB'), ('at', 'IN'), ('the', 'DT'), ('page', 'NN'), ('of', 'IN'), ('your', 'PRP$'), ('favorites', 'NNS'), ('.', '.'), ('See', 'VB'), ('your', 'PRP$'), ('stories', 'NNS'), ('and', 'CC'), ('back', 'RB'), ('at', 'IN'), ('Back', 'NNP'), ('of', 'IN'), ('this', 'DT'), ('story', 'NN'), ('.', '.'), ('Back', 'NNP'), ('and', 'CC'), ('back', 'RB'), ('by', 'IN'), ('the', 'DT'), ('Back', 'NNP'), ('to', 'TO'), ('it', 'PRP'), (',', ','), ('back', 'RB'), ('in', 'IN'), ('the', 'DT'), ('page', 'NN'), (':', ':'), ('See', 'VB'), ('back', 'RB'), ('at', 'IN'), ('it', 'PRP'), (',', ','), ('or', 'CC'), ('back', 'RB'), ('to', 'TO'), ('it', 'PRP'), ('.', '.'), ('Back.com', 'NNP'), (\"''\", \"''\"), ('Back', 'NNP'), ('to', 'TO'), ('them', 'PRP'), ('.', '.'), ('See', 'VB'), ('the', 'DT'), ('Back', 'NNP'), ('.', '.'), ('Back', 'NNP'), (\"'\", 'POS'), ('Back', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('page', 'NN'), (',', ','), ('back', 'RB'), ('and', 'CC'), ('back.com', 'NN'), (\"''\", \"''\"), ('Back', 'RB'), ('to', 'TO'), ('the', 'DT'), ('Story', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Click', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Dash', 'NNP'), ('Dash', 'NNP'), ('Dash.com', 'NNP'), ('.', '.'), (\"''\", \"''\"), ('Back', 'RB'), ('to', 'TO'), ('page', 'NN'), ('.', '.'), ('``', '``'), ('The', 'DT'), ('Back', 'NNP'), ('of', 'IN'), ('Back', 'NNP'), ('of', 'IN'), ('The', 'DT'), ('Back', 'NNP'), ('of', 'IN'), ('i-Dunk.com', 'NN'), (',', ','), (\"''\", \"''\"), ('back', 'RB'), ('to', 'TO'), ('i-dunk', 'NN'), ('.', '.'), ('Back', 'NNP'), (\"''\", \"''\"), ('``', '``'), ('Back', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Dink.com', 'NNP'), ('``', '``'), ('Back', 'NNP'), ('of', 'IN'), ('2009', 'CD'), (\"''\", \"''\"), (\"''\", \"''\")]\n",
      "Final Summary with POS tagging: Bayesian optimization is an approach to optimizing objective functions that take a long time to evaluate . It is best-suited for optimization over continuous domains of less than 20-dimensions . BayesOpt has been used extensively for engineering systems since the 1960s . The ability to optimize expensive black box functions makes it extremely popular for testing hyperparameters in machine learning . We conclude with a discussion of Bayesian optimization software and future research directions in the ﬁeld.com tutorial . Our focus is on �nding a global rather than local optimum , our focus on the BayesOpt is on the black box.com guide to Bayes Optimization.com/Bayes Optimism.com 's `` Bayesoptimism '' and our guide to the guide to more advanced techniques , which we will use in the guide.com-to-planning.com launch of the guide , which will be available on July 10 , 2018.com.com : Back to Back to the page you came from the back of the Back of the Web.com page : Share your memories . Back to back to the back to your memories of the page.com . Back of your memories , back of your stories , back to back of it.com and back of them . Back of them , and back to Back of it , and your memories and back stories . See back to us . Back at the bottom of the story . Back to your stories .. com , back at the page of your favorites . See your stories and back at Back of this story . Back and back by the Back to it , back in the page : See back at it , or back to it . Back.com '' Back to them . See the Back . Back ' Back of the page , back and back.com '' Back to the Story of the Click of the Dash Dash Dash.com . '' Back to page . `` The Back of Back of The Back of i-Dunk.com , '' back to i-dunk . Back '' `` Back of the Dink.com `` Back of 2009 '' ''\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(summary)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Display POS tags\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "\n",
    "# Optionally, reconstruct the summary with sentences (if needed)\n",
    "structured_summary = ' '.join(tokens)\n",
    "print(\"Final Summary with POS tagging:\", structured_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b37a7-dbc3-4c5b-8ad1-1f576372252e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866824ac-cf3a-490d-bebf-acfa7905f434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
